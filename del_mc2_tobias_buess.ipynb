{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importiere Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuesst1\u001b[0m (\u001b[33mt_buess\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb -qqq\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, densenet201, ResNet50_Weights, DenseNet201_Weights\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import Tuple, List\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tkinter\n",
    "import tkinter.filedialog\n",
    "from tqdm.auto import tqdm\n",
    "import inspect\n",
    "import sys\n",
    "import ipywidgets\n",
    "import functools\n",
    "\n",
    "#append directory to path variable\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "user = \"t_buess\"\n",
    "project = \"del_mc2_rev2\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.abspath(\"\") + \"\\\\del_mc2_tobias_buess.ipynb\" \n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp():\n",
    "    import torch\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from PIL import Image\n",
    "    from imgaug import augmenters as iaa\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import copy\n",
    "    from tqdm import tqdm \n",
    "\n",
    "    class Tokenizer:\n",
    "        def __init__(self, min_token_count:int=3) -> None:\n",
    "            \"\"\" Diese Klasse ist für die Tokenisierung der Captions zuständig\n",
    "            Args:\n",
    "                min_token_count (int): minimale Vorkommenshäufigkeit für einen bestimmten Token\n",
    "            \"\"\"\n",
    "            \n",
    "            self.tok_to_index = {\"<SOS>\":0, \"<EOS>\":1, \"<PAD>\":2} #token zu index\n",
    "            self.index_to_token = [\"<SOS>\", \"<EOS>\", \"<PAD>\"] #index zu token\n",
    "            self.min_token_count = min_token_count \n",
    "        \n",
    "        def build_vocab(self, corpus:list) -> None:\n",
    "            \"\"\"Generiere Vokabular aus korpus\n",
    "            Args:\n",
    "                corpus (list): Liste von Strings\n",
    "            \"\"\"\n",
    "\n",
    "            i_start = len(self.index_to_token) #setze start i\n",
    "\n",
    "            token_count = {} #hier werden die Vorkommnisse der Tokens gezählt\n",
    "\n",
    "            #build vocab\n",
    "            for element in corpus:\n",
    "                for token in Tokenizer.tokenize_text(element): #tokenisiere\n",
    "                    if token not in self.index_to_token: #falls token noch nicht registriert\n",
    "                        token_count[token] = token_count.get(token, 0) + 1 #addiere token Vorkommnis\n",
    "\n",
    "                        if token_count[token] >= self.min_token_count: #überprüfe ob minimale Anzahl erreicht\n",
    "                            self.tok_to_index[token] = i_start #speichere token\n",
    "                            self.index_to_token.append(token) #speichere token\n",
    "                            i_start += 1\n",
    "\n",
    "        def numericalize(self, caption:str) -> torch.Tensor:\n",
    "            \"\"\"Konvertiere caption zu vektor aus indices\n",
    "            Args:\n",
    "                caption (str): caption\n",
    "\n",
    "            Returns:\n",
    "                caption als Tensor\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.tensor([self.tok_to_index[token] for token in Tokenizer.tokenize_text(caption) if token in self.index_to_token]) #iteriere über tokens, mache zu numerics und gebe als Tensor zurück\n",
    "        \n",
    "        def numerical_to_matrix(self, numeric_caption:torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"Generiert onehot encodings aus Output von `numericalize`\n",
    "            Args:\n",
    "                numeric_caption (torch.Tensor): Tensor, welcher durch 'numericalize' erstellt wurde\n",
    "\n",
    "            Returns:\n",
    "                Onehot encoding als Tensor\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.zeros((list(numeric_caption.shape) + [len(self)])).scatter_(len(numeric_caption.shape), numeric_caption.unsqueeze(len(numeric_caption.shape)).type(torch.int64), 1)\n",
    "        \n",
    "        def oneHot_sequence_to_tokens(self, sequence:torch.Tensor):\n",
    "            \"\"\"Inverse von `numerical_to_matrix`\n",
    "            Args:\n",
    "                sequence (torch.Tensor): onehot encodings\n",
    "\n",
    "            Returns:\n",
    "                liste von tokens als string\n",
    "            \"\"\"\n",
    "            \n",
    "            argmax = sequence.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            tokens = []\n",
    "            for arg in argmax:\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "        \n",
    "        def numerical_to_tokens(self, sequence:torch.Tensor):\n",
    "            \"\"\" Inverse von `numericalize`\n",
    "            Args:\n",
    "                sequence (torch.Tensor): sequenz von numerischen tokens (output von `oneHot_sequence_to_tokens`)\n",
    "\n",
    "            Returns:\n",
    "                liste von tokens als string\n",
    "            \"\"\"\n",
    "            \n",
    "            tokens = []\n",
    "            for arg in sequence.numpy():\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def __len__(self):\n",
    "            \"\"\"Länge des vocabs\n",
    "            \"\"\"\n",
    "            return len(self.index_to_token)\n",
    "            \n",
    "        @staticmethod\n",
    "        def tokenize_text(text:str) -> list:\n",
    "            \"\"\"Konvertiert text zu einer liste aus tokens\n",
    "            \"\"\"\n",
    "            return [token for token in text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).strip().split(\" \")]\n",
    "\n",
    "    class FlickrDataset(Dataset):\n",
    "        def __init__(self, img_root_dir:str, captions_file:str, img_transform=None, train_amount:float=0.8, valid_amount:float=0.1, split_random_state:int=1234, min_token_count:int=3, use_augmentation:bool=False) -> None:\n",
    "            \"\"\" Diese Klasse repräsentiert die Schnittstelle zwischen Dataloader und der Eigentlichen Daten.\n",
    "            Zusätzlich können noch Transformationen, sowie train-val-test ratio bestimmt werden und data augmentation auf den trainingsdaten aktiviert werden.\n",
    "            \n",
    "            Args:\n",
    "                img_root_dir (str): root directory vom Bilderordner\n",
    "                captions_file (str): filename des cpation files\n",
    "                img_transform: transformation auf allen Bildern\n",
    "                train_amount (float): grösse des training-splits\n",
    "                valid_amount (float): grösse des validation-splits (1 - train_amount - valid_amount ist die grösse des test-splits)\n",
    "                split_random_state (int): seed des splits\n",
    "                min_token_count (int): jeder Token muss mindestens so vielmal vorkommen, damit dieser ins Vokabular aufgenommen wird.\n",
    "                use_augmentation (bool): True -> Aktiviere Data-augmentation auf dem Trainings-split\n",
    "            \"\"\"\n",
    "            \n",
    "            super().__init__()\n",
    "            self.img_root_dir = img_root_dir #ordner mit bilder\n",
    "            self.captions_file = captions_file #file mit captions\n",
    "            self.img_transform = img_transform #transformation der bilder\n",
    "\n",
    "            self.captions = pd.read_csv(self.captions_file, header=0, names=[\"img\", \"caption\"]) #lese die captions ein\n",
    "\n",
    "            #instanziere tokenizer und erstelle vocab\n",
    "            self.tokenizer = Tokenizer(min_token_count=min_token_count)\n",
    "            self.tokenizer.build_vocab(self.captions.caption.values)\n",
    "\n",
    "            self.captions = self.captions.groupby(\"img\").caption.apply(list).to_frame().reset_index() #gruppiere nach den images, da jedes Image mehrere captions besitzt\n",
    "\n",
    "            #make train validation test split\n",
    "            self.train_captions = self.captions.sample(frac=train_amount, random_state=split_random_state) #sample train\n",
    "            self.valid_caption = self.captions.drop(self.train_captions.index).sample(frac=1/(1-train_amount)*valid_amount, random_state=split_random_state) #sample validation\n",
    "            self.test_caption = self.captions.drop(self.train_captions.index).drop(self.valid_caption.index) #sample test\n",
    "\n",
    "            #totale länge\n",
    "            tot_len = len(self.train_captions) + len(self.valid_caption) + len(self.test_caption)\n",
    "\n",
    "            print(f\"Dataset was fractioned into:\\n- train: {len(self.train_captions)/tot_len}\\n- validation: {len(self.valid_caption)/tot_len}\\n- test: {len(self.test_caption)/tot_len}\")\n",
    "            \n",
    "            self.captions = self.train_captions.explode(\"caption\") #überschreibe captions mit trainings captions\n",
    "\n",
    "            self.use_augmentation = use_augmentation #init\n",
    "\n",
    "            self.is_test = False #only set if test\n",
    "\n",
    "        def get_validation(self):\n",
    "            \"\"\"Get validation split\n",
    "            Returns:\n",
    "                copy of self\n",
    "            \"\"\"\n",
    "            \n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.valid_caption.explode(\"caption\") #overrdie captions with valid_caption explosion\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            return ds_c\n",
    "        \n",
    "        def get_test(self):\n",
    "            \"\"\"Get test split\n",
    "            Returns:\n",
    "                copy of self\n",
    "            \"\"\"\n",
    "\n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.test_caption #overrdie captions with test_captions\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            ds_c.is_test = True #set test to true\n",
    "\n",
    "            return ds_c\n",
    "                \n",
    "        def augmentate_image(self, img:np.array):\n",
    "            \"\"\"Augmentiert bild mit zufälligen verschiedenen transformationen wie (blur, flip, rotation, pixel dropping, hue & saturation)\n",
    "            \"\"\"\n",
    "            aug = iaa.Sequential([\n",
    "                iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(rotate=(-20, 20), mode='symmetric'), \n",
    "                iaa.Sometimes(0.5,\n",
    "                            iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                        iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
    "                iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "            ])\n",
    "\n",
    "            return aug.augment_image(img) \n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.captions)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            #get caption(s) and image id from dataframe\n",
    "            img_id, caption = self.captions.iloc[idx]\n",
    "\n",
    "            #read image from storage\n",
    "            img = Image.open(os.path.join(self.img_root_dir, img_id))\n",
    "\n",
    "            #if augmentation enabled -> do augmentation\n",
    "            if self.use_augmentation:\n",
    "                img = Image.fromarray(self.augmentate_image(np.array(img)))\n",
    "            \n",
    "            #if transformation enabled\n",
    "            if self.img_transform is not None:\n",
    "                img = self.img_transform(img)\n",
    "\n",
    "            if self.is_test: #caption is a list of captions\n",
    "                return img, caption\n",
    "            \n",
    "            else: #it is a single caption\n",
    "                numericalized_caption = torch.cat((torch.tensor([self.tokenizer.tok_to_index[\"<SOS>\"]]), self.tokenizer.numericalize(caption), torch.tensor([self.tokenizer.tok_to_index[\"<EOS>\"]])))\n",
    "                return img, numericalized_caption\n",
    "        \n",
    "    class Collate:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            \"\"\" Fügt paddings zu captions hinzu, damit alle captions innerhalb eines batches gleich gross sind\n",
    "            \"\"\"\n",
    "            \n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch]\n",
    "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx) #füge paddings hinzu\n",
    "\n",
    "            targets = self.dataset.tokenizer.numerical_to_matrix(targets) #sequenz aus tokens (ids) zu onehot\n",
    "\n",
    "            return imgs, targets\n",
    "        \n",
    "    class Collate_Test:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch] #verändere targets nicht\n",
    "\n",
    "            return imgs, targets\n",
    "    \n",
    "    def get_loader(img_root_dir, captions_file, img_transform=None, batch_size=64, train_amount=0.8, valid_amount=0.1, min_token_count=3, use_augmentation=False, num_workers_train:int=16):\n",
    "        \"\"\" Erstellt datensets und dataloaders\n",
    "        Args:\n",
    "            img_root_dir (str): root directory vom Bilderordner\n",
    "            captions_file (str): filename des cpation files\n",
    "            img_transform: transformation auf allen Bildern\n",
    "            batch_size (int): grösse eines batches\n",
    "            train_amount (float): grösse des training-splits\n",
    "            valid_amount (float): grösse des validation-splits (1 - train_amount - valid_amount ist die grösse des test-splits)\n",
    "            min_token_count (int): jeder Token muss mindestens so vielmal vorkommen, damit dieser ins Vokabular aufgenommen wird.\n",
    "            use_augmentation (bool): True -> Aktiviere Data-augmentation auf dem Trainings-split\n",
    "            num_workers_train (int): workers des train dataloaders\n",
    "\n",
    "        Returns:\n",
    "            train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        #erstelle datenset\n",
    "        train_dataset = FlickrDataset(img_root_dir, captions_file, img_transform, train_amount=train_amount, valid_amount=valid_amount, min_token_count=min_token_count, use_augmentation=use_augmentation)\n",
    "        \n",
    "        #hole validations dataset\n",
    "        valid_dataset = train_dataset.get_validation()\n",
    "\n",
    "        #hole test datenset\n",
    "        test_dataset = train_dataset.get_test()\n",
    "\n",
    "        #erstelle dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(train_dataset),\n",
    "            num_workers=num_workers_train,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        validation_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(valid_dataset),\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate_Test(test_dataset),\n",
    "        )\n",
    "\n",
    "        return train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "#temporary outsource dataset stuff into file (otherwise num_workers of trainloader wont't work)\n",
    "with open(f'./tmp_ksdbf97skd.py', 'w') as file:\n",
    "    file.write(\"\\n\".join([line[4:] for line in inspect.getsource(tmp).split(\"\\n\")[1:]]))\n",
    "\n",
    "from tmp_ksdbf97skd import Tokenizer, FlickrDataset, Collate, Collate_Test, get_loader #use outsourced function and classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse: Tokenizer\n",
    "\n",
    "Der Tokenizer ist für die Tokenisierung der Captions zuständig.\n",
    "\n",
    "Die Funktion **buid_vocab** erstellt dabei das Vokabular.\n",
    "\n",
    "Dieses definiert, welche Wörter bzw. Tokens das Modell kennt, um unbekannte Bilder zu beschreiben.\n",
    "\n",
    "Diese Funktion nimmt dabei eine Liste von strings, in diesem Fall die einzelnen Captions an.\n",
    "\n",
    "Das Vokabular wird auf allen Captions (auch diese im Test und Validation) gebildet, sodass es während des Trainings zu keinem Fehler kommt.\n",
    "\n",
    "Bei der Tokenisierung **tokenize_text** werden folgende Schritte ausgeführt:\n",
    "1. alles wird zu lowercase geändert\n",
    "2. entferne punctuation nach *string.punctuation* \n",
    "3. entferne whitespaces links und rechts vom Text\n",
    "4. aufspalten in einzelne Wörter\n",
    "5. Versuche die Wörter zum Vokabular hinzuzufügen, falls die Mindestanzahl eines Wortes erreicht und sich noch nicht im Vokabular befindet.\n",
    "\n",
    "Nach dem erstellen des Vokabulars kann durch das Ausführen folgender Funktionen ein Text zu einer Abfolge von Tokens transformiert werden:\n",
    "\n",
    "*text* -> **numericalize** -> **numerical_to_matrix** -> *tensor shape(Anzahl Tokens im Text, vocab length)*\n",
    "\n",
    "Unbekannte tokens werden ignoriert.\n",
    "\n",
    "Eine Abfolge von Tokens kann auch wieder zu einer Abfolge von Strings zurück transformiert werden:\n",
    "\n",
    "*tensor shape(Anzahl Tokens, vocab length)* -> **oneHot_sequence_to_tokens** -> **numerical_to_tokens**\n",
    "\n",
    "## Klasse: FlickrDataset\n",
    "\n",
    "Die Klasse *FlickrDataset* ist die custom Klasse für die Flickr Daten.\n",
    "\n",
    "Wird diese Klasse instanziert, so werden die Daten von einem bestimmten Ort eingelesen. Der Ort kann angepasst werden.\n",
    "\n",
    "Bei der Initialisierung der Instanz wird auch ein Train-Validation-Test split durchgeführt werden. Das Verhältnis kann angepasst werden.\n",
    "\n",
    "Zusätzlich kann man eine Data-augmentation für die Trainingsdaten aktivieren. Diese kann über einen Parameter gesteuert werden.\n",
    "\n",
    "Wird die Data-augmentation aktiviert, so werden auf den Trainigsdaten zufällige Transformationen durchgeführt. Dies soll Overfitting etwas dämpfen. (Für Details bitte **augmentate_image** betrachten)\n",
    "\n",
    "Wir nun die Instanz einem Dataloader übergeben, so wird der Trainings-teil der Daten verwendet.\n",
    "\n",
    "Möchte man zu den Validierungs oder Testdaten gelangen, so kann dies durch die Methoden **get_validation** oder **get_test** erreicht werden.\n",
    "\n",
    "Diese fertigen eine Kopie der FlickrDataset-Instanz, verwenden allerdings den entsprechenden Split der Daten.\n",
    "\n",
    "Zusätzlich wird bei diesen auch die Data-augmentation ausgeschaltet, sollte diese Aktiv sein.\n",
    "\n",
    "Wird nun **__getitem__** ausgeführt, so wird das Bild mit der dazugehörigen Caption geladen als Tuple (Bild als Tensor, Tokenisierte Caption als Tensor).\n",
    "\n",
    "*Hinweis:*\n",
    "\n",
    "Bei der Caption wird beim Start noch der *Start-Token* und am Ende einen *End-Token* hinzugefügt.\n",
    "\n",
    "Damit kann später das Netz initialisiert werden bzw. das Ende eines Satzes erkennen.\n",
    "\n",
    "Der Datensatz von **get_test** ist bezüglich der Struktur der Daten etwas anders.\n",
    "\n",
    "Hier wird beim Ausführen von **__getitem__** die Caption nicht mehr als Tokenisierte Sequenz, sondern alle unbearbeiteten Captions für dieses Bild zurückgegeben.\n",
    "\n",
    "Dadurch kann das Modell mittels bleu score einfacher getestet werden.\n",
    "\n",
    "## Klasse: Collate / Collate_Test\n",
    "\n",
    "Diese Klassen werden für das Funktionieren der Dataloader benötigt.\n",
    "\n",
    "Diese sorgt dafür, dass bei eim Laden eines Batches alle Sequenzen der Captions gleich lange sind, indem diese mit dem Padding-token aufgefüllt werden.\n",
    "\n",
    "## Weshalb muss dieser Code ausgelagert werden\n",
    "\n",
    "Um das Laden der Daten zu beschläunigen, ist es von Vorteil, wenn die *num_workers* der Dataloader > 0 ist.\n",
    "\n",
    "Diese Funktionalität funktioniert in Windows allerdings nicht, wenn sich der Code im Jupyter befindet.\n",
    "\n",
    "Deshalb wird der Code ausgelagert und Importiert, da dieser Workaround funktioniert."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2) -> None:\n",
    "        \"\"\" Diese Klasse repräsentiert einen Teil des Netzes und enthält das LSTM.\n",
    "        Es können die Embeddings, sowie die Weight und Biases des LSTM und die Weights, welche die Embeddings zurück in den Token space zurück transformieren, trainiert werden.\n",
    "\n",
    "        Args:\n",
    "            embedding_size (int): Grösse des Embeddings der Token\n",
    "            hidden_size (int): Grösse des LSTM cell states sowie hidden state\n",
    "            vocab_size (int): Grösse des Vokabulars (für die Rücktransformierung in den Token space)\n",
    "            dropout (float): Dropout rate des Dropout layers innerhalb des LSTMs\n",
    "            num_layers (int): Anzahl layer innerhalb des LSTMs\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(vocab_size, embedding_size) #embeddings stored here\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) #final output\n",
    "\n",
    "    def forward(self, image_embedding:torch.Tensor, onehot_words:torch.Tensor):\n",
    "        \"\"\" Forward propagation für Training\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embeddign für Bild (dim: batchsize x embedding_size)\n",
    "            onehot_words (torch.Tensor): sequenz für onehot tokens (dim: batchsize x sequence len x vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            output der Sequenz mit dimensionen (batchsize x 1 [wegen Bild] + sequence len x embedding size)\n",
    "        \"\"\"\n",
    "\n",
    "        image_embedding = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "        word_embeddings = self.embedding(onehot_words) #embed onehot words\n",
    "\n",
    "        embeddings = torch.cat((image_embedding, word_embeddings), dim=1) #concat image embedding and word embeddings along sequence dim \n",
    "\n",
    "        x, _ = self.lstm(embeddings) #forward durch lstm\n",
    "        x = self.fc(x) #transformiere zurück von embedding size zu onehot size\n",
    "        x = F.log_softmax(x, dim=2) #wegen nlloss\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_greedy(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=10):\n",
    "        \"\"\"Captioning eines Bildes mit greedy Methode. Sobald der stop token gefunden wurde, wird das captioning gestoppt.\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embedding des Bilds (dim: 1 x embedding_size)\n",
    "            start_token_onehot (torch.Tensor): one hot encoding des start tokens (dim: onehot size)\n",
    "            stop_token_onehot (torch.Tensor): one hot encoding des stop tokens (dim: onehot size)\n",
    "            max_num_words (int): maximale länge der caption\n",
    "        \n",
    "        Returns:\n",
    "            vorhergesage tokens als tensor\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = [] #predicted words stored here\n",
    "\n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            _, one_hot, c = self._step_next_token(self.embedding(start_token_onehot.unsqueeze(0).unsqueeze(0)), c) #feed embedding of start token (add first two dimension to start token) into network\n",
    "\n",
    "            #if stop token predicted\n",
    "            if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                if len(prediction) == 0:\n",
    "                    return torch.empty([])\n",
    "            \n",
    "                return torch.stack(prediction)\n",
    "        \n",
    "            prediction.append(one_hot[0, 0, :]) #add first predicted word to prediction list\n",
    "\n",
    "            for i in range(max_num_words):\n",
    "                input = self.embedding(one_hot) #embed word\n",
    "\n",
    "                _, one_hot, c = self._step_next_token(input, c) # predict next token based on last token and last c\n",
    "\n",
    "                #if stop token predicted\n",
    "                if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                    if len(prediction) == 0:\n",
    "                        return torch.empty([])\n",
    "                \n",
    "                    return torch.stack(prediction)\n",
    "                \n",
    "                prediction.append(one_hot[0, 0, :])\n",
    "\n",
    "            return torch.stack(prediction)\n",
    "\n",
    "    def sample_beamSearch(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\"Captioning eines Bildes mit beamsearch Methode.\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embedding des Bilds (dim: 1 x embedding_size)\n",
    "            start_token_onehot (torch.Tensor): one hot encoding des start tokens (dim: onehot size)\n",
    "            stop_token_onehot (torch.Tensor): one hot encoding des stop tokens (dim: onehot size)\n",
    "            max_num_words (int): maximale länge der caption\n",
    "            beam_size (int): grösse des Beams\n",
    "            \n",
    "        Returns:\n",
    "            vorhergesage tokens als tensor\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            global_best_prob = torch.tensor(-torch.inf) #best probability found stored here\n",
    "            \n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            def beamsearch(last_token_oneHot:torch.Tensor, last_prob:float, c:tuple, n:int=0) -> Tuple[List[int], float]:\n",
    "                nonlocal global_best_prob #global best probability\n",
    "\n",
    "                n += 1 #add n\n",
    "\n",
    "                input = self.embedding(last_token_oneHot)#embed\n",
    "\n",
    "                #forward pass through network\n",
    "                x, c = self.lstm(input, c) \n",
    "                x = x[:, [-1], :]\n",
    "                x = self.fc(x)\n",
    "                x = torch.log_softmax(x, dim=2) #log softmax\n",
    "\n",
    "                best_k_idx = torch.topk(x, beam_size, dim=2, largest=True).indices.flatten() #get indices of best predictions\n",
    "                best_k_prob = x[0, 0, best_k_idx] #get log probabilities of best predictions\n",
    "\n",
    "                #iterate over best prediction (idx with corresponding prob)\n",
    "                best_prediction = None\n",
    "                for idx, prob_t in torch.stack((best_k_idx, best_k_prob), dim=1):\n",
    "                    prob_current = last_prob+prob_t #calculate new probability \n",
    "\n",
    "                    #stop searching if prob_current already more negative than currently best -> because it can only get more negative or better say worse\n",
    "                    if prob_current < global_best_prob:\n",
    "                        continue\n",
    "                    \n",
    "                    #if we land here a token found which describes the image better\n",
    "                    idx_as_onehot = torch.zeros_like(x).scatter_(2, idx.unsqueeze(0).unsqueeze(0).unsqueeze(0).type(torch.int64), 1) #calculate onehot encoded token\n",
    "\n",
    "                    #if stop token predicted or max number of terms reached\n",
    "                    if torch.equal(idx_as_onehot[0, 0, :], stop_token_onehot) or n > max_num_words:\n",
    "                        #new best scorer found\n",
    "                        global_best_prob = prob_current\n",
    "                        best_prediction = []\n",
    "\n",
    "                    else:\n",
    "                        best_tokens_next = beamsearch(idx_as_onehot, prob_current, c, n) #end token not found yet\n",
    "\n",
    "                        #only returns != None if a better caption found\n",
    "                        if best_tokens_next != None:\n",
    "                            best_prediction = [idx] + best_tokens_next #append token to this caption\n",
    "\n",
    "                return best_prediction\n",
    "\n",
    "            best_prediction = beamsearch(start_token_onehot.unsqueeze(0).unsqueeze(0), 0, c, 0) #feed embedding of start token (add first two dimension to start token) into network -> best prediction\n",
    "\n",
    "            return torch.Tensor(best_prediction).flatten().type(torch.int)\n",
    "\n",
    "    def sample_beamSearch_v2(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\"Captioning eines Bildes mit beamsearch Methode.\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embedding des Bilds (dim: 1 x embedding_size)\n",
    "            start_token_onehot (torch.Tensor): one hot encoding des start tokens (dim: onehot size)\n",
    "            stop_token_onehot (torch.Tensor): one hot encoding des stop tokens (dim: onehot size)\n",
    "            max_num_words (int): maximale länge der caption\n",
    "            beam_size (int): grösse des Beams\n",
    "            \n",
    "        Returns:\n",
    "            vorhergesage tokens als tensor\n",
    "        \"\"\"\n",
    "\n",
    "        def get_best_k_tokens(log_softmax:torch.Tensor):\n",
    "            \"\"\"Get token with its log probability \n",
    "            Args:\n",
    "                log_softmax (torch.tensor): log softmax of token prediction\n",
    "\n",
    "            Returns:\n",
    "                list[(log prob, token), (log prob, token), ...]\n",
    "            \"\"\"\n",
    "            \n",
    "            best_k_idx = torch.topk(log_softmax, beam_size, dim=2, largest=True).indices.flatten() #get indices of best k predictions\n",
    "            \n",
    "            prob_and_tok = []\n",
    "            for idx in best_k_idx:\n",
    "                log_prob = log_softmax.flatten()[idx].item()\n",
    "                onehot = torch.zeros_like(log_softmax).scatter_(2, idx.unsqueeze(0).unsqueeze(0).unsqueeze(0).type(torch.int64), 1) #calculate onehot encoded token\n",
    "\n",
    "                prob_and_tok.append((log_prob, onehot))\n",
    "\n",
    "            return prob_and_tok\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            #make first prediction with start token\n",
    "            log_softmax, _, c = self._step_next_token(self.embedding(start_token_onehot.unsqueeze(0).unsqueeze(0)), c) #predict from start token and image\n",
    "            best_k_tokens = get_best_k_tokens(log_softmax) #get best k tokens\n",
    "\n",
    "            #setup beam list\n",
    "            currently_best_beams = [] #currently best beams logged here with following structured: [(last log prob, last c, [token sequence]), ...]\n",
    "            for log_prob, onehot in best_k_tokens:\n",
    "                beam = (log_prob, c, [onehot])\n",
    "                currently_best_beams.append(beam)\n",
    "\n",
    "            #iterate over beams \n",
    "            for i in range(max_num_words - 1):\n",
    "                new_beams = []\n",
    "                beams_changed = False\n",
    "                for one_best_beam in currently_best_beams:\n",
    "                    old_beam_log_prob = one_best_beam[0]\n",
    "                    old_beam_c = one_best_beam[1]\n",
    "                    old_beam_token_sequence = one_best_beam[2]\n",
    "\n",
    "                    #if stop token predicted during last run\n",
    "                    if torch.equal(old_beam_token_sequence[-1][0, 0, :], stop_token_onehot):\n",
    "                        #do nothing and keep this beam as it is\n",
    "                        new_beams.append(one_best_beam)\n",
    "\n",
    "                    else: #stop token not predicted\n",
    "                        beams_changed = True #set flag\n",
    "\n",
    "                        #predict next k tokens and append beams\n",
    "                        log_softmax, _, c = self._step_next_token(self.embedding(old_beam_token_sequence[-1]), old_beam_c) #predict from newest token and newest c\n",
    "                        best_k_tokens = get_best_k_tokens(log_softmax) #get best k tokens\n",
    "\n",
    "                        for log_prob, onehot in best_k_tokens:\n",
    "                            beam = (old_beam_log_prob + log_prob, c, old_beam_token_sequence + [onehot])\n",
    "                            new_beams.append(beam)\n",
    "\n",
    "                #interrupt if beams didn't change anymore (all beams contain stop token)\n",
    "                if not beams_changed:\n",
    "                    break\n",
    "\n",
    "                currently_best_beams = sorted(new_beams, key=lambda x: x[0] + np.log(len(x[2])), reverse=True)[:beam_size] #get k best beams with highes log probability and replace old ones\n",
    "\n",
    "            best_beam_tokens = sorted(currently_best_beams, key=lambda x: x[0] + np.log(len(x[2])), reverse=True)[0][2] #get tokens of beam with highest probability\n",
    "\n",
    "            return torch.cat([token for token in best_beam_tokens if not torch.equal(token[0, 0, :], stop_token_onehot)], 1).argmax(dim=2).flatten().type(torch.int32).cpu() #concat (but remove stop token if available) and return arg tokens\n",
    "\n",
    "    def _step_next_token(self, input:torch.Tensor, c:tuple=None):\n",
    "        \"\"\" Predicted nächsten token \n",
    "        Args:\n",
    "            input (torch.Tensor): embedded token (dim: 1 x sequence_len x embedding_size)\n",
    "            c (tuple): hidden state\n",
    "\n",
    "        Returns:\n",
    "            (log softmax des predicteten tokens, one hot encoded token, hidden state c)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x, c = self.lstm(input, c) \n",
    "            x = x[:, [-1], :]\n",
    "            x = self.fc(x)\n",
    "            max_idx = torch.argmax(x, dim=2)\n",
    "            one_hot = torch.zeros_like(x).scatter_(2, max_idx.unsqueeze(2), 1)\n",
    "\n",
    "            return F.log_softmax(x, dim=2), one_hot, c\n",
    "\n",
    "def make_cnn_resnet50(embedding_size:int):\n",
    "    \"\"\"Make resnet50 with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.fc = nn.Sequential(nn.BatchNorm1d(4*512), nn.Linear(4*512, embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_cnn_densenet201(embedding_size:int):\n",
    "    \"\"\"Make densenet with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.classifier = nn.Sequential(nn.BatchNorm1d(cnn.classifier.weight.shape[1]), nn.Linear(cnn.classifier.weight.shape[1], embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_rnn_lstm(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    \"\"\"Make lstm \n",
    "    \"\"\"\n",
    "    return RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "\n",
    "def make_rnn_gru(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    \"\"\"Make rnn with lstm replaced with gru\n",
    "    \"\"\"\n",
    "    gru = RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "    gru.lstm = nn.GRU(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #replace lstm with gru layer\n",
    "    return gru"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse RNN:\n",
    "\n",
    "Die Klasse *RNN* ist ein Teil des gesamten Netzes und enthält das LSTM.\n",
    "\n",
    "Wird diese Klasse instanziert, so kann über Parameter die Grösse des Embeddings, Cell state sowie hidden State eingestellt werden.\n",
    "\n",
    "Die Methode **forward** wird dabei nur während des Trainings für die Teacher-force Methode verwendet.\n",
    "\n",
    "Bei der Teacher-force Methode wird der Ground-truth (in diesem Fall die korrekte Caption) zu einem früheren Zeitpunkt als Input verwendet.\n",
    "\n",
    "Damit kann das Modell effizienter und auch schneller Lernen, da somit keine For-loops verwendet werden müssen.\n",
    "\n",
    "Dabei wird die Sequenz der Tokens *onehot_words* (beginnt mit start token, gefolgt der eigentlichen tokenisierten caption und endet mit einem stop Token und optionalen padding Tokens) zuerst durch den Embedding layer durchgeschoben.\n",
    "\n",
    "Wichtig zu erwähnen ist, dass die einzelnen Tokens onehot Vektoren sind, bei welchen an der Stelle vom entsprechenden Wort im Vokabular eine eins steht.\n",
    "\n",
    "Nachdem Embedden der Sequenz haben die Embeddings der Tokens also die gleiche Grösse wie das Embedding des Bildes.\n",
    "\n",
    "Anschliessend wird das Embedding des Bildes und die Embeddings der Tokens Konkateniert und in das LSTM geführt.\n",
    "\n",
    "Der Output des LSTMs sind nun die Predictions an der entsprechenden Stelle der hineingeführen Sequenz und haben die gleiche Grösse wie die *hidden_size*.\n",
    "\n",
    "Anschliessend werden die Outputs durch einen Linearen Layer wieder zurück in den Token space (grösse des Vokabulars) zurücktransformiert.\n",
    "\n",
    "Durch ein argmax des Outputs kann schlussendlich herausgefunden werden, welches Wort das LSTM predicted hat.\n",
    "\n",
    "In **forward** wird allerdings ein log_softmax verwendet, damit nll_loss verwendet werden kann.\n",
    "\n",
    "Zusätzlich bietet die *RNN* Klasse die Methode **sample_greedy** sowie **sample_beamSearch**.\n",
    "\n",
    "Diese beiden Methoden sind für das Eigentliche captioning verantwortlich.\n",
    "\n",
    "Währenddessen bei **forward** die Teacher-force Methode verwendet wird, wird bei diesen beiden Methoden das nächste Wort aufgrund der bereits vorhergesagten Wörtern das nächste Predicted.\n",
    "\n",
    "Bei **sample_greedy**  wird folgender Approach verwendet:\n",
    "1. Das Embedding des Bildes wird dem LSTM zugeführt. Dadurch erhalten wir eine Vorhersage für den nächsten Token sowie der cell und hidden state. Die Vorhersage für den nächsten Token wird hier allerdings nicht benötigt. Die Informationen des Bildes sind zu diesem Zeitpunkt im Hidden und Cell-state Abgebildet.\n",
    "2. Nun soll das LSTM den ersten Token predicten. Dazu wird der start Token sowie der Cell und Hidden-State als Input verwendet. Nun erhalten wir das erste Wort und einen neuen Cell/Hidden-state.\n",
    "3. Nun wird der soeben predictete Token und neue Cell/Hidden-state in das Modell hineingegeben. Wir erhalten den nächsten Token und aktualisierte Cell/Hidden-state.\n",
    "4. Der Schritt 3 wird so lange wiederholt, bis entweder der Stop token predicted wird (dies sagt das Ende der Caption voraus) oder eine maximale Anzahl an Tokens predicted wurde.\n",
    "5. Gebe vorhergesagte Tokens zurück.\n",
    "\n",
    "Bei **sample_beamSearch** wird prinzipiell derselbe Ablauf verwendet.\n",
    "\n",
    "Allerdings wird nicht mehr der wahrscheinlichste Token (argmax) als nächster Token verwendet, sondern die nächsten k wahrscheinlichsten Token verwendet.\n",
    "\n",
    "Dabei wird die wahrscheinlichste Prediction schlussendlich zurück gegeben.\n",
    "\n",
    "Leider hat diese Methode, so wie sie umgesetzt ist, den Nachteil, dass diese kurze Predictions bevorzugt.\n",
    "\n",
    "Für Details, bitte Kommentare im Code betrachten.\n",
    "\n",
    "**sample_beamSearch_v2** ist eine weitere Implementierung des Beamsearch-algorithmus.\n",
    "\n",
    "Folgendes Beispiel soll die Funktionsweise des Algorithmus erklären:\n",
    "1. Das Embeddede Bild wird in das LSTM eingefügt und der ersten k Token aufgrund des Start-tokens predicted. Die besten k Token werden aufgrund der grösse der log probability (log softmax) ausgewählt.\n",
    "2. Nun werden wiederum die besten k tokens der bereits gefundenen k tokens berechnet. Ist k beispielsweise 3, so bekommen wir nun 9 (3 x 3) sequenzen.\n",
    "3. Nun werden die k Sequenzen ausgewählt, welche die grössen kombinierten log Probabilities besitzen (die log prob. des ersten und zweiten token wird dabei addiert). Nun besitzen wir wieder k Sequenzen.\n",
    "4. Auf diesen Sequenzen werden wieder k predictions gemacht usw., bis entweder die maximale token Anzahl oder der End-token erreicht wurde. \n",
    "5. Sobald der search fertig ist, wird die Sequenz mit der grössten log Probability als Vorschlag ausgewählt. \n",
    "\n",
    "Teilweise bevorzugt dieser Algorithmus wie **sample_beamSearch** eher kürzere Sequenzen, weshalb der erklärte Algorithmus so modifiziert wurde, dass er längere Sequenzen nicht mehr bestraft.\n",
    "\n",
    "Dies wurde mit dem Hinzuaddieren von dem Logarithmus der Länge der Sequenz zur totalen Probability einer Sequenz erreicht.\n",
    "\n",
    "\n",
    "## **make_cnn_resnet50** & **make_cnn_densenet201**\n",
    "\n",
    "Um Embeddings der Bilder erstellen zu können, werden vortrainierte CNNs verwendet.\n",
    "\n",
    "Dabei werden alle Weights & Biases innerhalb des CNN während des Trainings nicht mehr verändert.\n",
    "\n",
    "Ledinglich der letzte Layer wird ersetzt und auch mit trainiert.\n",
    "\n",
    "Der letzte Layer besteht aus einem Batchnorm-layer, gefolgt von einem Linearen-layer, gefolgt von einem Batchnorm-layer.\n",
    "\n",
    "Die Anzahl der Nodes im Linearen layer kann angepasst werden und bestimmt die grösse des Embeddings des Bildes.\n",
    "\n",
    "Die Batchnorm-layer habe ich hinzugefügt in der Hoffnung, dass das Modell etwas schneller trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, train_dataset:FlickrDataset, embedding_size:int=100, hidden_size:int=100, lstm_dropout:float=0, num_rnn_layers:int=2, cnn_type:str=\"resnet50\", rnn_type:str=\"lstm\", test_eval_step_pred_max_sequLen:int=30, test_step_pred_beamsize:int=3, alpha:float=0.0001):\n",
    "        \"\"\"Lightning modul des Netzes\n",
    "        Args:\n",
    "            train_dataset (FlickrDataset): Datenset (hier wird allerdings nur der Tokenizer benötigt)\n",
    "            embedding_size (int): grösse der embeddings\n",
    "            hidden_size (int): grösse der hidden size (cell state / hidden state)\n",
    "            lstm_dropout (float): dropout probability between lstm layers (if num_lstm_layers > 1)\n",
    "            num_lstm_layers (int): anzahl lstm layers\n",
    "            cnn_type (str): typ des image encoders ('resnet50' oder 'densenet201')\n",
    "            rnn_type (str): typ des rnn ('lstm' oder 'gru')\n",
    "            test_eval_step_pred_max_sequLen (int): max länge der prediction einer caption während des tests sowie validierung\n",
    "            test_step_pred_beamsize (int): beam size des beamsearch während des tests\n",
    "            alpha (float): regularisierung\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.test_eval_step_pred_max_sequLen = test_eval_step_pred_max_sequLen\n",
    "        self.test_step_pred_beamsize = test_step_pred_beamsize\n",
    "        self.tokenizer = train_dataset.tokenizer\n",
    "        self.tokenizer_min_token_count = self.tokenizer.min_token_count\n",
    "        self.data_augmentation_enabled = train_dataset.use_augmentation\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #load specific cnn\n",
    "        if cnn_type == \"resnet50\":\n",
    "            self.cnn = make_cnn_resnet50(embedding_size)\n",
    "\n",
    "        elif cnn_type == \"densenet201\":\n",
    "            self.cnn = make_cnn_densenet201(embedding_size)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"unknown cnn_type\")\n",
    "        \n",
    "        #load specific rnn\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = make_rnn_lstm(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_rnn_layers)\n",
    "\n",
    "        elif rnn_type == \"gru\":\n",
    "            self.rnn = make_rnn_gru(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_rnn_layers)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"unknown rnn_type\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Training step für lightning\n",
    "        \"\"\"\n",
    "\n",
    "        loss = self._get_teacherForce_loss(batch) #calculate loss based on batch\n",
    "         \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True) #log loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Validation step für lightning\n",
    "        \"\"\"\n",
    "\n",
    "        # loss_greedy not logged anymore since the bleu loss doesn't get much better after a couple epochs (and started to get very wiggly) and uses much compute time\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_tf = self._get_teacherForce_loss(batch) #calculate teacher force log loss based on batch\n",
    "            #loss_greedy = self._get_greedy_bleu_loss(batch) #calculate greedy log loss based on batch\n",
    "        \n",
    "        self.log(\"val_loss\", loss_tf, on_step=True, on_epoch=True) #negative log loss of teacher forcing\n",
    "        #self.log(\"val_loss_bleu\", loss_greedy, on_step=True, on_epoch=True) #bleu loss of greedy prediction\n",
    "\n",
    "    def test_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Test step für lightning (bleu score)\n",
    "        \"\"\"\n",
    "        batch_image, batch_captions = batch #split des inputs\n",
    "\n",
    "        list_bleu_greedy = [] #hier werden die bleu scores mit greedy für alle bilder des batches abgespeichert\n",
    "        list_bleu_beamSearch = [] #hier werden die bleu scores mit beamsearch für alle bilder des batches abgespeichert\n",
    "        list_bleu_beamSearch_v2 = [] #hier werden die bleu scores mit beamsearch v2 für alle bilder des batches abgespeichert\n",
    "        #iteriere über batch\n",
    "        for i in range(len(batch_image)):\n",
    "            image = batch_image[i] #hole einzelnes bild\n",
    "            captions = [self.tokenizer.tokenize_text(caption) for caption in batch_captions[i]] #tokenize die einzelnen captions des bildes\n",
    "\n",
    "            #führe ein captioning des bildes durch. Einmal mit greedy einmal mit beamsearch und einmal mit beamsearch v2\n",
    "            pred_greedy = self.sample_greedy(image, self.test_eval_step_pred_max_sequLen)\n",
    "            pred_beamsearch = self.sample_beamSearch(image, self.test_eval_step_pred_max_sequLen, self.test_step_pred_beamsize)\n",
    "            pred_beamsearch_v2 = self.sample_beamSearch_v2(image, self.test_eval_step_pred_max_sequLen, self.test_step_pred_beamsize)\n",
    "\n",
    "            #bleu scores berechnen\n",
    "            bleu_greedy = sentence_bleu(captions, pred_greedy)\n",
    "            bleu_beamSearch = sentence_bleu(captions, pred_beamsearch)\n",
    "            bleu_beamSearch_v2 = sentence_bleu(captions, pred_beamsearch_v2)\n",
    "\n",
    "            #store in list\n",
    "            list_bleu_greedy.append(bleu_greedy)\n",
    "            list_bleu_beamSearch.append(bleu_beamSearch)\n",
    "            list_bleu_beamSearch_v2.append(bleu_beamSearch_v2)\n",
    "\n",
    "        #logge mittelwert\n",
    "        self.log(\"test_bleu_greedy\", np.mean(list_bleu_greedy), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu greedy\n",
    "        self.log(\"test_bleu_beamSearch\", np.mean(list_bleu_beamSearch), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu beamsearch\n",
    "        self.log(\"test_bleu_beamSearch_v2\", np.mean(list_bleu_beamSearch_v2), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu beamsearch v2\n",
    "\n",
    "    def _get_teacherForce_loss(self, batch:tuple):\n",
    "        \"\"\"Calculate loss (teacher forced) based on batch\n",
    "        Args:\n",
    "            batch (tuple): output von dataloader\n",
    "\n",
    "        Returns:\n",
    "            negative log loss von batch\n",
    "        \"\"\"\n",
    "\n",
    "        #type definition\n",
    "        batch_images:torch.Tensor\n",
    "        batch_targets_onehot:torch.Tensor\n",
    "\n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding = self.cnn(batch_images) #get image embeddings for whole batch\n",
    "\n",
    "        output:torch.Tensor = self.rnn(image_embedding, batch_targets_onehot[:, :-1, :]) #feed image and tokens into rnn (last word is not sent to rnn because this would the end token [if no paddings at the end])\n",
    "\n",
    "        output = output[:, 1:, :] #ignore first sequence from output since this is the prediction of the image (this prediction does not add value to the model why its ignored)\n",
    "        batch_targets_onehot = batch_targets_onehot[:, 1:, :] #ignore first sequence from tokens since this is the start token\n",
    "\n",
    "        #remove batch dimension\n",
    "        batch_targets_onehot = batch_targets_onehot.flatten(end_dim=1)\n",
    "        output = output.flatten(end_dim=1)\n",
    "        \n",
    "        #nll loss with padding tokens ignored\n",
    "        loss = F.nll_loss(output, batch_targets_onehot.argmax(dim=1), ignore_index=int(self.tokenizer.tok_to_index[\"<PAD>\"]))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _get_greedy_bleu_loss(self, batch:tuple):\n",
    "        \"\"\"Calculate bleu loss (simply 1 - bleu score).\n",
    "        Args:\n",
    "            batch (tuple): output von dataloader\n",
    "\n",
    "        Returns:\n",
    "            mittlerer bleu loss des batches\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding:torch.Tensor = self.cnn(batch_images) #get image embeddings for whole batch and add a sequence dimension\n",
    "\n",
    "        #get start and stop token\n",
    "        start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "        stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "        #iterate over batch\n",
    "        bleu_losses = [] #bleu losses stored here\n",
    "        for i in range(len(batch_images)):\n",
    "            image:torch.Tensor = image_embedding[[i]] #get image from batch\n",
    "            targets_onehot:torch.Tensor = batch_targets_onehot[i] #get image from batch\n",
    "            \n",
    "            #transform onehot tokens back to string tokens (ignore special tokens)\n",
    "            reference = list(filter(lambda a: a not in ['<SOS>', '<EOS>', '<PAD>'], self.tokenizer.oneHot_sequence_to_tokens(targets_onehot)))\n",
    "            \n",
    "            #predict string tokens from greedy prediction\n",
    "            prediction = self.tokenizer.oneHot_sequence_to_tokens(self.rnn.sample_greedy(image, start_token_onehot, stop_token_onehot, self.test_eval_step_pred_max_sequLen))\n",
    "\n",
    "            #add bleu loss to list\n",
    "            bleu_losses.append(1-sentence_bleu([reference], prediction))\n",
    "        \n",
    "        return np.mean(bleu_losses) #return mean bleu loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), weight_decay=self.alpha)\n",
    "        return optimizer\n",
    "        \n",
    "    def sample_greedy(self, image:torch.Tensor, max_num_words:int=10):\n",
    "        \"\"\" Predict caption mit greedy methode\n",
    "        Args:\n",
    "            image (torch.Tensor): Bild als Tensor\n",
    "            max_num_words (int): maximale anzahl an tokens der prediction\n",
    "\n",
    "        Returns:\n",
    "            caption as string tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0)) #embedd image\n",
    "\n",
    "            #get start and stop token\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            #make a greedy prediction\n",
    "            output = self.rnn.sample_greedy(image_embedding, start_token_onehot, stop_token_onehot, max_num_words)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output.size()) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.oneHot_sequence_to_tokens(output.cpu()) #convert onehot tokens to string tokens\n",
    "        \n",
    "    def sample_beamSearch(self, image:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\" Predict caption mit beamsearch methode\n",
    "        Args:\n",
    "            image (torch.Tensor): Bild als Tensor\n",
    "            max_num_words (int): maximale anzahl an tokens der prediction\n",
    "            beam_size (int): beam size\n",
    "\n",
    "        Returns:\n",
    "            caption as string tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0))\n",
    "\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            output = self.rnn.sample_beamSearch(image_embedding, start_token_onehot, stop_token_onehot, max_num_words, beam_size)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.numerical_to_tokens(output)\n",
    "        \n",
    "    def sample_beamSearch_v2(self, image:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\" Predict caption mit beamsearch (version 2) methode\n",
    "        Args:\n",
    "            image (torch.Tensor): Bild als Tensor\n",
    "            max_num_words (int): maximale anzahl an tokens der prediction\n",
    "            beam_size (int): beam size\n",
    "\n",
    "        Returns:\n",
    "            caption as string tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0))\n",
    "\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            output = self.rnn.sample_beamSearch_v2(image_embedding, start_token_onehot, stop_token_onehot, max_num_words, beam_size)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.numerical_to_tokens(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse: LitModel\n",
    "\n",
    "Die Klasse *LitModel* kombiniert das CNN und das RNN.\n",
    "\n",
    "Dabei können verschiedene Parameter eingestellt werden, wie die grösse der Embeddings (*embedding_size*), grösse des Cell/Hidden-state des RNN (*hidden_size*) usw. (Siehe code für Details).\n",
    "\n",
    "Die Methode *training_step* berechnet dabei der nll_loss eines Batches mit der Teacher-force Methode und wird von Pytorch-lightning verwendet um das Modell zu trainieren.\n",
    "\n",
    "Die Methode *validation_step* tut dasselbe, allerdings für den Validierungs-datensatz.\n",
    "\n",
    "Die Methode *test_step* berechnet den bleu-score für einen Batch für jeweils die greedy_prediction und beamsearch_prediction.\n",
    "\n",
    "Die Methode **_get_teacherForce_loss** ist für die Berechnung des nll_loss für die Teacher-force Methode zuständig. Diese Methode kann auf ganzen Batches operieren.\n",
    "\n",
    "Dazu werden folgende Schritte ausgeführt:\n",
    "1. Alle Bilder des Batches werden durch das CNN Embedded. Dadurch erhalten wir die Embeddings der Bilder.\n",
    "2. Nun werden die Bilder und die Sequenzen von Caption-tokens in die rnn Instanz geführt. Bei den Sequenzen wird allerdings das letzte Element abgeschnitten, da wir die Sequenz mit dem Bild verlängert haben (dies passiert in der rnn Instanz). Nun erhalten wir den log_softmax für jeden vorhergesagten Token zu jeder Position der Eingeführten Sequenz für alle Elemente im Batch.\n",
    "3. Nun wird pro batch beim Output jeweils der erste predictete Token abgeschnitten, da dies der Vorhergesagte Token auf das Embedding des Bildes ist. Dies wäre eigentlich der Start token, welcher predicted werden müsste. Da uns allerdings der Start-token allerdings nicht interessiert, wird diese Prediction nicht in die Optimierung des Modelles einfliessen. Zusätzlich wird auch das erste Element der ground Truth Sequenz (Tokenisierte caption) abgeschitten, da es sich hier wie bereits erwähnt um den Start-token handelt.  \n",
    "4. Nun wir der ground Truth und Output geflattet und in die Loss-funtion eingefügt. Dabei werden die Positionen ignoriert, an welchen der Padding-token predicted werden müsse, da diese Tokens beim Predicten keinen Merwert erbringen, sprich diese für das Captioning nicht benötigt werden.\n",
    "\n",
    "Diese Methode wird beim Training und Validierungsschritt verwendet.\n",
    "\n",
    "Die Methode **_get_greedy_bleu_loss** wird verwendet um den bleu Loss zu berechnen.\n",
    "\n",
    "Die Methode kann auf Batches operieren.\n",
    "\n",
    "Der bleu Loss wird berechnet aus 1 - bleu score.\n",
    "\n",
    "Dazu werden folgende Schritte ausgeführt:\n",
    "1. Alle Bilder des Batches werden durch das CNN Embedded. Dadurch erhalten wir die Embeddings der Bilder.\n",
    "2. Anschliessend wird über jedes Bild iteriert und durch die **sample_greedy** Methode gespiesen. Dadurch erhalten wir die vorhergesagte Caption des Bildes als Liste von strings (greedy Methode).\n",
    "3. Anschliessend wird beim ground Truth die stop/end/padding-tokens entfernt.\n",
    "4. Nun wird die Vorhersage mit dem ground Truth verglichen und mittels **sentence_bleu** (Funktion von Library nltk) der bleu score Berechnet\n",
    "5. Nun wird für jedes Bild \"1 - bleu score\" berechnet und in einer Liste abgespeichert\n",
    "6. Zuletzt wird der Mittelwert dieser einzelnen bleu losses berechnet und als return value zurückgegeben\n",
    "\n",
    "Die Methoden **sample_greedy**, **sample_beamSearch** und **sample_beamSearch_v2** sind für die greedy/beamSearch prediction verantwortlich.\n",
    "\n",
    "Dazu wird lediglich das Embedding des Bildes generiert und anschliessend durch die Methoden von *RNN* zu einer Abfolge von Tokens (one hot Vektoren) transformiert (Prediction der Caption).\n",
    "\n",
    "Zu guter letzt werden die Vektoren wieder zu Worten zurücktransformiert und mittels return zurück gegeben."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv(r\"C:\\Users\\tobia\\archive\\captions.txt\")\n",
    "all_tokens = functools.reduce(lambda x,y: x + y, map(lambda x: Tokenizer.tokenize_text(x), captions.caption.values), [])\n",
    "\n",
    "tokens, token_counts = np.unique(all_tokens, return_counts=True)\n",
    "argsorted = np.argsort(token_counts)[::-1]\n",
    "tokens = tokens[argsorted]\n",
    "token_counts = token_counts[argsorted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 10))\n",
    "plt.bar(tokens[:50], token_counts[:50])\n",
    "plt.xticks(rotation = 45) # Rotates X-Axis Ticks by 45-degrees\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlabel(\"token\")\n",
    "plt.title(\"50 Tokens mit der grössten occurrence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(token_counts[::-1])\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Cumulative sum of token count\")\n",
    "plt.xlabel(\"token nr. (sorted)\")\n",
    "plt.ylabel(\"count (cumsum)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting auf Bruchteil der Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erstelle datenset (image resizing auf 224x224, keine augmentation)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((224, 224), antialias=True), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = FlickrDataset(r\"C:\\Users\\tobia\\archive\\Images\", r\"C:\\Users\\tobia\\archive\\captions.txt\", transform, train_amount=0.8, valid_amount=0.1, min_token_count=0, use_augmentation=False)\n",
    "train_dataset.captions = train_dataset.captions.iloc[:64, :] #behalte nur 64 observationen\n",
    "\n",
    "#erstelle dataloaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=Collate(train_dataset),\n",
    "    num_workers=os.cpu_count(),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "#erstelle modell (embedding/hidden size: 1024, num lstm layers: 1, cnn: resnet50, rnn: lstm, keine regularisierung)\n",
    "model = LitModel(train_dataset, 1024, 1024, 0, 1, \"resnet50\", \"lstm\", 0, 0, 0)\n",
    "\n",
    "#start training (auf 100 epochen)\n",
    "logger = WandbLogger(project=project, log_model=\"all\")\n",
    "logger.watch(model, log=\"all\")\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, log_every_n_steps=4, max_epochs=100)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was fractioned into:\n",
      "- train: 0.800024718823384\n",
      "- validation: 0.099987640588308\n",
      "- test: 0.099987640588308\n"
     ]
    }
   ],
   "source": [
    "##load data\n",
    "batchsize = 64\n",
    "min_token_count = 3\n",
    "use_augmentation = True\n",
    "img_resize = 224\n",
    "num_workers_train = os.cpu_count()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((img_resize, img_resize), antialias=True), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset = get_loader(r\"C:\\Users\\tobia\\archive\\Images\", r\"C:\\Users\\tobia\\archive\\captions.txt\", transform, batchsize, min_token_count=min_token_count, use_augmentation=use_augmentation, num_workers_train=num_workers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_train(embedding_size:int, hidden_size:int, lstm_dropout:float=0.5, num_rnn_layers:int=2, cnn_type:str=\"densenet201\", rnn_type:str=\"lstm\", test_step_pred_max_sequLen:int=30, test_step_pred_beamsize:int=3, alpha:float=0.0001):\n",
    "    \"\"\"Train a single run\n",
    "    Args:\n",
    "        embedding_size (int): embedding size\n",
    "        hidden_size (int): cell/hidden state size\n",
    "        lstm_dropout (float): dropout rate of dropout layer between lstm layers (if num_rnn_layers > 1)\n",
    "        num_rnn_layers (int): number of stacked lstm layers\n",
    "        cnn_type (str): type of cnn ('resnet50' or 'densenet201')\n",
    "        rnn_type (str): type of rnn ('lstm' or 'gru')\n",
    "        test_step_pred_max_sequLen (int): max sequence length of prediction in test step\n",
    "        test_step_pred_beamsize (int): beam size of beamsearch in test step\n",
    "        alpha (float): regularization parameter of adam\n",
    "    \"\"\"\n",
    "    \n",
    "    model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, num_rnn_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "    #start training\n",
    "    logger = WandbLogger(project=project, log_model=\"all\")\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "    trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "    best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "    wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "def sweep_iteration():\n",
    "    wandb.init()\n",
    "\n",
    "    #create fresh model\n",
    "    embedding_size = wandb.config.embedding_size\n",
    "    hidden_size = wandb.config.hidden_size\n",
    "    lstm_dropout = wandb.config.lstm_dropout\n",
    "    num_rnn_layers = wandb.config.num_rnn_layers\n",
    "    cnn_type = wandb.config.cnn_type\n",
    "    rnn_type = wandb.config.rnn_type\n",
    "    test_step_pred_max_sequLen = 30\n",
    "    test_step_pred_beamsize = 3\n",
    "    alpha = wandb.config.alpha\n",
    "\n",
    "    model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, num_rnn_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "    #start training\n",
    "    logger = WandbLogger(project=project, log_model=\"all\")\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=False, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "    trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "    best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "    wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model in a sweep\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"rough_tuning\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"minimize\",\n",
    "        \"name\": \"best_val_loss\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"embedding_size\": {\"values\":[128, 512, 1024]},\n",
    "        \"hidden_size\": {\"values\":[128, 512, 1024]},\n",
    "        \"lstm_dropout\": {\"values\":[0.5]},\n",
    "        \"num_rnn_layers\": {\"values\":[2]},\n",
    "        \"cnn_type\": {\"values\":[\"resnet50\", \"densenet201\"]},\n",
    "        \"rnn_type\": {\"values\":[\"lstm\"]},\n",
    "        \"alpha\": {\"values\":[0, 1E-2, 1E-4]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "wandb.agent(sweep_id, sweep_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-slvv0874:best, 201.82MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.3\n"
     ]
    }
   ],
   "source": [
    "##load model from checkpoint\n",
    "model_run_id = \"model-slvv0874:best\"\n",
    "artifact_dir = wandb.Api().artifact(f\"{user}/{project}/{model_run_id}\").download()\n",
    "\n",
    "# load checkpoint\n",
    "model = LitModel.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show multiple images\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "max_num_words = 30\n",
    "for i in range(0, 100):\n",
    "    image, captions = ds.__getitem__(i) #get item\n",
    "\n",
    "    imshow(image)\n",
    "    print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "    print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "    print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=10))\n",
    "    print(\"beamsearch caption (v2): \", model.sample_beamSearch_v2(image.cuda(), max_num_words=max_num_words, beam_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show single image\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "image, captions = ds.__getitem__(0) #get item\n",
    "\n",
    "imshow(image)\n",
    "print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=10))\n",
    "print(\"beamsearch caption (v2): \", model.sample_beamSearch_v2(image.cuda(), max_num_words=max_num_words, beam_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom image\n",
    "root = tkinter.Tk()\n",
    "root.withdraw()\n",
    "file_path = tkinter.filedialog.askopenfilename()\n",
    "root.destroy()\n",
    "\n",
    "image = Image.open(file_path)\n",
    "image = transform(image)\n",
    "\n",
    "imshow(image)\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=10))\n",
    "print(\"beamsearch caption (v2): \", model.sample_beamSearch_v2(image.cuda(), max_num_words=max_num_words, beam_size=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
