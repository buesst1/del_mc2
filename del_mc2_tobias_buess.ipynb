{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbuesst1\u001b[0m (\u001b[33mt_buess\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb -qqq\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, densenet201, ResNet50_Weights, DenseNet201_Weights\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import Tuple, List\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tkinter\n",
    "import tkinter.filedialog\n",
    "from tqdm import tqdm \n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "#append directory to path variable\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "user = \"t_buess\"\n",
    "project = \"del_mc2\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.abspath(\"\") + \"\\\\del_mc2_tobias_buess.ipynb\" \n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp():\n",
    "    import torch\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from PIL import Image\n",
    "    from imgaug import augmenters as iaa\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import copy\n",
    "    from tqdm import tqdm \n",
    "\n",
    "    class Tokenizer:\n",
    "        def __init__(self, min_token_count:int=3) -> None:\n",
    "            self.tok_to_index = {\"<SOS>\":0, \"<EOS>\":1, \"<PAD>\":2} #converts token to index\n",
    "            self.index_to_token = [\"<SOS>\", \"<EOS>\", \"<PAD>\"] #converts index to token\n",
    "            self.min_token_count = min_token_count #define min token count\n",
    "        \n",
    "        def build_vocab(self, corpus:list) -> None:\n",
    "            \"\"\"Generates vocab (tok_to_index, index_to_token) from corpus (captions)\n",
    "            \"\"\"\n",
    "\n",
    "            i_start = len(self.index_to_token) #set start i\n",
    "\n",
    "            token_count = {} #count tokens here\n",
    "\n",
    "            #build vocab\n",
    "            for element in corpus:\n",
    "                for token in Tokenizer.tokenize_text(element):\n",
    "                    if token not in self.index_to_token: #if token unknown to vocab\n",
    "                        token_count[token] = token_count.get(token, 0) + 1\n",
    "\n",
    "                        if token_count[token] >= self.min_token_count: #only add token to corpus if min count reached\n",
    "                            self.tok_to_index[token] = i_start\n",
    "                            self.index_to_token.append(token)\n",
    "                            i_start += 1\n",
    "\n",
    "        def numericalize(self, caption:str) -> torch.Tensor:\n",
    "            \"\"\"Konvertiere caption zu vektor\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.tensor([self.tok_to_index[token] for token in Tokenizer.tokenize_text(caption) if token in self.index_to_token]) #iteriere über tokens, mache zu numerics und gebe als Tensor zurück\n",
    "        \n",
    "        def numerical_to_matrix(self, numeric_caption:torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"Makes args to one hot encoded vectors (supports more dimensional numeric_caption tensors)\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.zeros((list(numeric_caption.shape) + [len(self)])).scatter_(len(numeric_caption.shape), numeric_caption.unsqueeze(len(numeric_caption.shape)).type(torch.int64), 1)\n",
    "        \n",
    "        def oneHot_sequence_to_tokens(self, sequence:torch.Tensor):\n",
    "            argmax = sequence.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            tokens = []\n",
    "            for arg in argmax:\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "        \n",
    "        def numerical_to_tokens(self, sequence:torch.Tensor):\n",
    "            tokens = []\n",
    "            for arg in sequence.numpy():\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.index_to_token)\n",
    "            \n",
    "        @staticmethod\n",
    "        def tokenize_text(text:str) -> list:\n",
    "            \"\"\"Converts text to list of tokens\n",
    "            \"\"\"\n",
    "            return [token for token in text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).strip().split(\" \")]\n",
    "\n",
    "    class FlickrDataset(Dataset):\n",
    "        def __init__(self, img_root_dir:str, captions_file:str, img_transform=None, train_amount:float=0.8, valid_amount:float=0.1, split_random_state:int=1234, min_token_count:int=3, train_augmentation_multiplier:int=0) -> None:\n",
    "            super().__init__()\n",
    "            self.img_root_dir = img_root_dir\n",
    "            self.captions_file = captions_file\n",
    "            self.img_transform = img_transform\n",
    "\n",
    "            self.captions = pd.read_csv(self.captions_file, header=0, names=[\"img\", \"caption\"])\n",
    "\n",
    "            self.tokenizer = Tokenizer(min_token_count=min_token_count)\n",
    "            self.tokenizer.build_vocab(self.captions.caption.values)\n",
    "\n",
    "            self.captions = self.captions.groupby(\"img\").caption.apply(list).to_frame().reset_index()\n",
    "\n",
    "            #make train validation test split\n",
    "            self.train_captions = self.captions.sample(frac=train_amount, random_state=split_random_state) #sample train\n",
    "            self.valid_caption = self.captions.drop(self.train_captions.index).sample(frac=1/(1-train_amount)*valid_amount, random_state=split_random_state) #sample validation\n",
    "            self.test_caption = self.captions.drop(self.train_captions.index).drop(self.valid_caption.index) #sample test\n",
    "\n",
    "            tot_len = len(self.train_captions) + len(self.valid_caption) + len(self.test_caption)\n",
    "\n",
    "            print(f\"Dataset was fractioned into:\\n- train: {len(self.train_captions)/tot_len}\\n- validation: {len(self.valid_caption)/tot_len}\\n- test: {len(self.test_caption)/tot_len}\")\n",
    "            \n",
    "            self.captions = self.train_captions.explode(\"caption\") #override captions with explosion\n",
    "\n",
    "            self.use_augmentation = False #init with false\n",
    "\n",
    "            #if augmentation enabled\n",
    "            if train_augmentation_multiplier > 0:\n",
    "                self.captions = pd.concat([self.captions]*train_augmentation_multiplier, axis=0) #duplicate dataframe\n",
    "                self.use_augmentation = True\n",
    "                print(f\"Data augmentation on training dataset enabled. Trainset is now {train_augmentation_multiplier} times its original size\")\n",
    "\n",
    "            self.is_test = False #only set if test\n",
    "\n",
    "        def get_validation(self):\n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.valid_caption.explode(\"caption\") #overrdie captions with valid_caption explosion\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            return ds_c\n",
    "        \n",
    "        def get_test(self):\n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.test_caption #overrdie captions with test_captions\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            ds_c.is_test = True #set test to true\n",
    "\n",
    "            return ds_c\n",
    "                \n",
    "        def augmentate_image(self, img:np.array):\n",
    "            aug = iaa.Sequential([\n",
    "                iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(rotate=(-20, 20), mode='symmetric'),\n",
    "                iaa.Sometimes(0.5,\n",
    "                            iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                        iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
    "                iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "            ])\n",
    "\n",
    "            return aug.augment_image(img) \n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.captions)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            #get caption(s) and image id from dataframe\n",
    "            img_id, caption = self.captions.iloc[idx]\n",
    "\n",
    "            #read image from storage\n",
    "            img = Image.open(os.path.join(self.img_root_dir, img_id))\n",
    "\n",
    "            #if augmentation enabled -> do augmentation\n",
    "            if self.use_augmentation:\n",
    "                img = Image.fromarray(self.augmentate_image(np.array(img)))\n",
    "            \n",
    "            #if transformation enabled\n",
    "            if self.img_transform is not None:\n",
    "                img = self.img_transform(img)\n",
    "\n",
    "            if self.is_test: #caption is a list of captions\n",
    "                return img, caption\n",
    "            \n",
    "            else: #it is a single caption\n",
    "                numericalized_caption = torch.cat((torch.tensor([self.tokenizer.tok_to_index[\"<SOS>\"]]), self.tokenizer.numericalize(caption), torch.tensor([self.tokenizer.tok_to_index[\"<EOS>\"]])))\n",
    "                return img, numericalized_caption\n",
    "        \n",
    "    class Collate:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch]\n",
    "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "            targets = self.dataset.tokenizer.numerical_to_matrix(targets)\n",
    "\n",
    "            return imgs, targets\n",
    "        \n",
    "    class Collate_Test:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch]\n",
    "\n",
    "            return imgs, targets\n",
    "    \n",
    "    def get_loader(img_root_dir, captions_file, img_transform=None, batch_size=64, train_amount=0.8, valid_amount=0.1, min_token_count=3, train_augmentation_multiplier=0, num_workers_train:int=16):\n",
    "        train_dataset = FlickrDataset(img_root_dir, captions_file, img_transform, train_amount=train_amount, valid_amount=valid_amount, min_token_count=min_token_count, train_augmentation_multiplier=train_augmentation_multiplier)\n",
    "        valid_dataset = train_dataset.get_validation()\n",
    "        test_dataset = train_dataset.get_test()\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(train_dataset),\n",
    "            num_workers=num_workers_train,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        validation_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(valid_dataset),\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate_Test(test_dataset),\n",
    "        )\n",
    "\n",
    "        return train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "#temporary outsource dataset stuff into file (otherwise num_workers wont't work)\n",
    "with open(f'./tmp_ksdbf97skd.py', 'w') as file:\n",
    "    file.write(\"\\n\".join([line[4:] for line in inspect.getsource(tmp).split(\"\\n\")[1:]]))\n",
    "\n",
    "from tmp_ksdbf97skd import Tokenizer, FlickrDataset, Collate, Collate_Test, get_loader #use outsourced function and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(vocab_size, embedding_size) #embeddings stored here\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) #final output\n",
    "\n",
    "    def forward(self, image_embedding:torch.Tensor, onehot_words:torch.Tensor):\n",
    "        \"\"\" image_embedding: batchsize x embedding_size\n",
    "            onehot_words: batchsize x sequence len x vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        image_embedding = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "        word_embeddings = self.embedding(onehot_words) #embed onehot words\n",
    "\n",
    "        embeddings = torch.cat((image_embedding, word_embeddings), dim=1) #concat image embedding and word embeddings along dim 1 \n",
    "\n",
    "        x, _ = self.lstm(embeddings)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_greedy(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=10):\n",
    "        \"\"\"Predicts onehot encoded tokens until prediction matches 'stop_token_onehot'\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            prediction = [] #predicted words stored here\n",
    "\n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            _, one_hot, c = self._step_next_token(self.embedding(start_token_onehot.unsqueeze(0).unsqueeze(0)), c) #feed embedding of start token (add first two dimension to start token) into network\n",
    "\n",
    "            #if stop token predicted\n",
    "            if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                if len(prediction) == 0:\n",
    "                    return torch.empty([])\n",
    "            \n",
    "                return torch.stack(prediction)\n",
    "        \n",
    "            prediction.append(one_hot[0, 0, :]) #add first predicted word to prediction list\n",
    "\n",
    "            for i in range(max_num_words):\n",
    "                input = self.embedding(one_hot) #embed word\n",
    "\n",
    "                _, one_hot, c = self._step_next_token(input, c) # predict next token based on last token and last c\n",
    "\n",
    "                #if stop token predicted\n",
    "                if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                    if len(prediction) == 0:\n",
    "                        return torch.empty([])\n",
    "                \n",
    "                    return torch.stack(prediction)\n",
    "                \n",
    "                prediction.append(one_hot[0, 0, :])\n",
    "\n",
    "            return torch.stack(prediction)\n",
    "\n",
    "    def sample_beamSearch(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        with torch.no_grad():\n",
    "            global_best_prob = torch.tensor(-torch.inf) #best probability found stored here\n",
    "            \n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            def beamsearch(last_token_oneHot:torch.Tensor, last_prob:float, c:tuple, n:int=0) -> Tuple[List[int], float]:\n",
    "                nonlocal global_best_prob\n",
    "\n",
    "                n += 1 #add n\n",
    "\n",
    "                input = self.embedding(last_token_oneHot)#embed\n",
    "\n",
    "                #forward pass through network\n",
    "                x, c = self.lstm(input, c) \n",
    "                x = x[:, [-1], :]\n",
    "                x = self.fc(x)\n",
    "                x = torch.log_softmax(x, dim=2) #log softmax\n",
    "\n",
    "                best_k_idx = torch.topk(x, beam_size, dim=2, largest=True).indices.flatten() #get indices of best predictions\n",
    "                best_k_prob = x[0, 0, best_k_idx] #get log probabilities of best predictions\n",
    "\n",
    "                #iterate over best prediction (idx with corresponding prob)\n",
    "                best_prediction = None\n",
    "                for idx, prob_t in torch.stack((best_k_idx, best_k_prob), dim=1):\n",
    "                    prob_current = last_prob+prob_t #calculate new probability \n",
    "\n",
    "                    #stop searching if prob_current already more negative than currently best -> because it can only get more negative or better say worse\n",
    "                    if prob_current < global_best_prob:\n",
    "                        continue\n",
    "                    \n",
    "                    #if we land here a token found which describes the image better\n",
    "                    idx_as_onehot = torch.zeros_like(x).scatter_(2, idx.unsqueeze(0).unsqueeze(0).unsqueeze(0).type(torch.int64), 1) #calculate onehot encoded token\n",
    "\n",
    "                    #if stop token predicted or max number of terms reached\n",
    "                    if torch.equal(idx_as_onehot[0, 0, :], stop_token_onehot) or n > max_num_words:\n",
    "                        #new best scorer found\n",
    "                        global_best_prob = prob_current\n",
    "                        best_prediction = []\n",
    "\n",
    "                    else:\n",
    "                        best_tokens_next = beamsearch(idx_as_onehot, prob_current, c, n) #end token not found yet\n",
    "\n",
    "                        #only returns != None if a better caption found\n",
    "                        if best_tokens_next != None:\n",
    "                            best_prediction = [idx] + best_tokens_next #append token to this caption\n",
    "\n",
    "                return best_prediction\n",
    "\n",
    "            best_prediction = beamsearch(start_token_onehot.unsqueeze(0).unsqueeze(0), 0, c, 0) #feed embedding of start token (add first two dimension to start token) into network -> best prediction\n",
    "\n",
    "            return torch.Tensor(best_prediction).flatten().type(torch.int)\n",
    "\n",
    "    def _step_next_token(self, input:torch.Tensor, c:tuple=None):\n",
    "        \"\"\"input: 1 x sequence_len x embedding_size\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x, c = self.lstm(input, c) \n",
    "            x = x[:, [-1], :]\n",
    "            x = self.fc(x)\n",
    "            max_idx = torch.argmax(x, dim=2)\n",
    "            one_hot = torch.zeros_like(x).scatter_(2, max_idx.unsqueeze(2), 1)\n",
    "\n",
    "            return F.log_softmax(x, dim=2), one_hot, c\n",
    "\n",
    "def make_cnn_resnet50(embedding_size:int):\n",
    "    \"\"\"Make resnet50 with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.fc = nn.Sequential(nn.BatchNorm1d(4*512), nn.Linear(4*512, embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_cnn_densenet201(embedding_size:int):\n",
    "    \"\"\"Make densenet with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.classifier = nn.Sequential(nn.BatchNorm1d(cnn.classifier.weight.shape[1]), nn.Linear(cnn.classifier.weight.shape[1], embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_rnn_lstm(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    return RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "\n",
    "def make_rnn_gru(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    gru = RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "    gru.lstm = nn.GRU(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #replace lstm with gru layer\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, train_dataset:FlickrDataset, embedding_size:int=100, hidden_size:int=100, lstm_dropout:float=0, num_lstm_layers:int=2, cnn_type:str=\"resnet50\", rnn_type:str=\"lstm\", test_eval_step_pred_max_sequLen:int=30, test_step_pred_beamsize:int=3, alpha:float=0.0001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.test_eval_step_pred_max_sequLen = test_eval_step_pred_max_sequLen\n",
    "        self.test_step_pred_beamsize = test_step_pred_beamsize\n",
    "        self.tokenizer = train_dataset.tokenizer\n",
    "        self.tokenizer_min_token_count = self.tokenizer.min_token_count\n",
    "        self.data_augmentation_enabled = train_dataset.use_augmentation\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #load specific cnn\n",
    "        if cnn_type == \"resnet50\":\n",
    "            self.cnn = make_cnn_resnet50(embedding_size)\n",
    "\n",
    "        elif cnn_type == \"densenet201\":\n",
    "            self.cnn = make_cnn_densenet201(embedding_size)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"unknown cnn_type\")\n",
    "        \n",
    "        #load specific rnn\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = make_rnn_lstm(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_lstm_layers)\n",
    "\n",
    "        elif rnn_type == \"gru\":\n",
    "            self.rnn = make_rnn_gru(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_lstm_layers)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"unknown rnn_type\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_teacherForce_loss(batch) #calculate loss based on batch\n",
    "         \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True) #log loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # loss_greedy not logged anymore since the bleu loss doesn't get much better after a couple epochs and uses much compute time\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_tf = self._get_teacherForce_loss(batch) #calculate teacher force log loss based on batch\n",
    "            #loss_greedy = self._get_greedy_bleu_loss(batch) #calculate greedy log loss based on batch\n",
    "        \n",
    "        self.log(\"val_loss\", loss_tf, on_step=True, on_epoch=True) #negative log loss of teacher forcing\n",
    "        #self.log(\"val_loss_bleu\", loss_greedy, on_step=True, on_epoch=True) #bleu loss of greedy prediction\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        batch_image, batch_captions = batch\n",
    "\n",
    "        list_bleu_greedy = []\n",
    "        list_bleu_beamSearch = []\n",
    "        for i in range(len(batch_image)):\n",
    "            image = batch_image[i]\n",
    "            captions = [self.tokenizer.tokenize_text(caption) for caption in batch_captions[i]]\n",
    "\n",
    "            pred_greedy = self.sample_greedy(image, self.test_eval_step_pred_max_sequLen)\n",
    "            pred_beamsearch = self.sample_beamSearch(image, self.test_eval_step_pred_max_sequLen, self.test_step_pred_beamsize)\n",
    "\n",
    "            bleu_greedy = sentence_bleu(captions, pred_greedy)\n",
    "            bleu_beamSearch = sentence_bleu(captions, pred_beamsearch)\n",
    "\n",
    "            #store in list\n",
    "            list_bleu_greedy.append(bleu_greedy)\n",
    "            list_bleu_beamSearch.append(bleu_beamSearch)\n",
    "\n",
    "        self.log(\"test_bleu_greedy\", np.mean(list_bleu_greedy), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu greedy\n",
    "        self.log(\"test_bleu_beamSearch\", np.mean(list_bleu_beamSearch), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu greedy\n",
    "\n",
    "    def _get_teacherForce_loss(self, batch):\n",
    "        \"\"\"Calculate loss based on batch\n",
    "        \"\"\"\n",
    "        #type definition\n",
    "        batch_images:torch.Tensor\n",
    "        batch_targets_onehot:torch.Tensor\n",
    "\n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding = self.cnn(batch_images) #get image embeddings for whole batch\n",
    "\n",
    "        output:torch.Tensor = self.rnn(image_embedding, batch_targets_onehot[:, :-1, :]) #feed image and tokens into rnn (last word is not sent to rnn since this is only the stop token)\n",
    "\n",
    "        output = output[:, 1:, :] #ignore first sequence from output since this is the prediction if the image\n",
    "        batch_targets_onehot = batch_targets_onehot[:, 1:, :] #ignore first sequence from tokens since this is the start token\n",
    "\n",
    "        #remove batch dimension\n",
    "        batch_targets_onehot = batch_targets_onehot.flatten(end_dim=1) #ignore start token\n",
    "        output = output.flatten(end_dim=1)\n",
    "\n",
    "        loss = F.nll_loss(output, batch_targets_onehot.argmax(dim=1), ignore_index=int(self.tokenizer.tok_to_index[\"<PAD>\"]))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _get_greedy_bleu_loss(self, batch):\n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding:torch.Tensor = self.cnn(batch_images) #get image embeddings for whole batch and add a sequence dimension\n",
    "\n",
    "        start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "        stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "        #iterate over batch\n",
    "        bleu_losses = []\n",
    "        for i in range(len(batch_images)):\n",
    "            image:torch.Tensor = image_embedding[[i]]\n",
    "            targets_onehot:torch.Tensor = batch_targets_onehot[i]\n",
    "            \n",
    "            reference = list(filter(lambda a: a not in ['<SOS>', '<EOS>', '<PAD>'], self.tokenizer.oneHot_sequence_to_tokens(targets_onehot)))\n",
    "            \n",
    "            prediction = self.tokenizer.oneHot_sequence_to_tokens(self.rnn.sample_greedy(image, start_token_onehot, stop_token_onehot, self.test_eval_step_pred_max_sequLen))\n",
    "\n",
    "            bleu_losses.append(1-sentence_bleu([reference], prediction))\n",
    "        \n",
    "        return np.mean(bleu_losses)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), weight_decay=self.alpha)\n",
    "        return optimizer\n",
    "        \n",
    "    def sample_greedy(self, image:torch.Tensor, max_num_words:int=10):\n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0))\n",
    "\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            output = self.rnn.sample_greedy(image_embedding, start_token_onehot, stop_token_onehot, max_num_words)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output.size()) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.oneHot_sequence_to_tokens(output.cpu())\n",
    "        \n",
    "    def sample_beamSearch(self, image:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0))\n",
    "\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            output = self.rnn.sample_beamSearch(image_embedding, start_token_onehot, stop_token_onehot, max_num_words, beam_size)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.numerical_to_tokens(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset was fractioned into:\n",
      "- train: 0.800024718823384\n",
      "- validation: 0.099987640588308\n",
      "- test: 0.099987640588308\n",
      "Data augmentation on training dataset enabled. Trainset is now 6 times its original size\n"
     ]
    }
   ],
   "source": [
    "##load data\n",
    "batchsize = 64\n",
    "min_token_count = 3\n",
    "train_augmentation_multiplier = 6\n",
    "img_resize = 224\n",
    "num_workers_train = 8\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((img_resize, img_resize), antialias=True), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset = get_loader(r\"C:\\Users\\tobia\\archive\\Images\", r\"C:\\Users\\tobia\\archive\\captions.txt\", transform, batchsize, min_token_count=min_token_count, train_augmentation_multiplier=train_augmentation_multiplier, num_workers_train=num_workers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load model from checkpoint\n",
    "model_run_id = \"model-if28cok5:best\"\n",
    "artifact_dir = wandb.Api().artifact(f\"{user}/{project}/{model_run_id}\").download()\n",
    "\n",
    "# load checkpoint\n",
    "model = LitModel.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model in a single run\n",
    "\n",
    "#create fresh model\n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "lstm_dropout = 0.5\n",
    "lstm_num_layers = 3\n",
    "cnn_type = \"densenet201\"\n",
    "rnn_type = \"lstm\"\n",
    "test_step_pred_max_sequLen = 30\n",
    "test_step_pred_beamsize = 3\n",
    "alpha = 0.0001\n",
    "\n",
    "model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, lstm_num_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "#start training\n",
    "logger = WandbLogger(project=project, log_model=\"all\")\n",
    "logger.watch(model, log=\"all\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model in a sweep\n",
    "\n",
    "def sweep_iteration():\n",
    "    wandb.init()\n",
    "\n",
    "    #create fresh model\n",
    "    embedding_size = wandb.config.embedding_size\n",
    "    hidden_size = wandb.config.hidden_size\n",
    "    lstm_dropout = wandb.config.lstm_dropout\n",
    "    lstm_num_layers = wandb.config.lstm_num_layers\n",
    "    cnn_type = wandb.config.cnn_type\n",
    "    rnn_type = wandb.config.rnn_type\n",
    "    test_step_pred_max_sequLen = 30\n",
    "    test_step_pred_beamsize = 3\n",
    "    alpha = wandb.config.alpha\n",
    "\n",
    "    model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, lstm_num_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "    #start training\n",
    "    logger = WandbLogger(project=project, log_model=\"all\")\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10, verbose=False, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "    trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "    best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "    wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"test_sweep\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"minimize\",\n",
    "        \"name\": \"best_val_loss\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"embedding_size\": {\"values\":[16, 128, 512]},\n",
    "        \"hidden_size\": {\"values\":[16, 128, 512]},\n",
    "        \"lstm_dropout\": {\"values\":[0.5]},\n",
    "        \"lstm_num_layers\": {\"values\":[3]},\n",
    "        \"cnn_type\": {\"values\":[\"resnet50\", \"densenet201\"]},\n",
    "        \"rnn_type\": {\"values\":[\"lstm\"]},\n",
    "        \"alpha\": {\"values\":[0.0001]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "wandb.agent(sweep_id, sweep_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show multiple images\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "max_num_words = 30\n",
    "for i in range(0, 100):\n",
    "    image, captions = ds.__getitem__(i) #get item\n",
    "\n",
    "    imshow(image)\n",
    "    print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "    print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "    print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show single image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "image, captions = ds.__getitem__(18) #get item\n",
    "\n",
    "imshow(image)\n",
    "print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "root = tkinter.Tk()\n",
    "root.withdraw()\n",
    "file_path = tkinter.filedialog.askopenfilename()\n",
    "root.destroy()\n",
    "\n",
    "image = Image.open(file_path)\n",
    "image = transform(image)\n",
    "\n",
    "imshow(image)\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
