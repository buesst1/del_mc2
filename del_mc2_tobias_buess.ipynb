{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade wandb -qqq\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, densenet201, ResNet50_Weights, DenseNet201_Weights\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from imgaug import augmenters as iaa\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from typing import Tuple, List\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import tkinter\n",
    "import tkinter.filedialog\n",
    "from tqdm import tqdm \n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "#append directory to path variable\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "user = \"t_buess\"\n",
    "project = \"del_mc2\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.abspath(\"\") + \"\\\\del_mc2_tobias_buess.ipynb\" \n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp():\n",
    "    import torch\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from torch.utils.data import DataLoader, Dataset\n",
    "    from PIL import Image\n",
    "    from imgaug import augmenters as iaa\n",
    "    import pandas as pd\n",
    "    import string\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import copy\n",
    "    from tqdm import tqdm \n",
    "\n",
    "    class Tokenizer:\n",
    "        def __init__(self, min_token_count:int=3) -> None:\n",
    "            \"\"\" Diese Klasse ist für die Tokenisierung der Captions zuständig\n",
    "            Args:\n",
    "                min_token_count (int): minimale Vorkommenshäufigkeit für einen bestimmten Token\n",
    "            \"\"\"\n",
    "            \n",
    "            self.tok_to_index = {\"<SOS>\":0, \"<EOS>\":1, \"<PAD>\":2} #token zu index\n",
    "            self.index_to_token = [\"<SOS>\", \"<EOS>\", \"<PAD>\"] #index zu token\n",
    "            self.min_token_count = min_token_count \n",
    "        \n",
    "        def build_vocab(self, corpus:list) -> None:\n",
    "            \"\"\"Generiere Vokabular aus korpus\n",
    "            Args:\n",
    "                corpus (list): Liste von Strings\n",
    "            \"\"\"\n",
    "\n",
    "            i_start = len(self.index_to_token) #setze start i\n",
    "\n",
    "            token_count = {} #hier werden die Vorkommnisse der Tokens gezählt\n",
    "\n",
    "            #build vocab\n",
    "            for element in corpus:\n",
    "                for token in Tokenizer.tokenize_text(element): #tokenisiere\n",
    "                    if token not in self.index_to_token: #falls token noch nicht registriert\n",
    "                        token_count[token] = token_count.get(token, 0) + 1 #addiere token Vorkommnis\n",
    "\n",
    "                        if token_count[token] >= self.min_token_count: #überprüfe ob minimale Anzahl erreicht\n",
    "                            self.tok_to_index[token] = i_start #speichere token\n",
    "                            self.index_to_token.append(token) #speichere token\n",
    "                            i_start += 1\n",
    "\n",
    "        def numericalize(self, caption:str) -> torch.Tensor:\n",
    "            \"\"\"Konvertiere caption zu vektor aus indices\n",
    "            Args:\n",
    "                caption (str): caption\n",
    "\n",
    "            Returns:\n",
    "                caption als Tensor\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.tensor([self.tok_to_index[token] for token in Tokenizer.tokenize_text(caption) if token in self.index_to_token]) #iteriere über tokens, mache zu numerics und gebe als Tensor zurück\n",
    "        \n",
    "        def numerical_to_matrix(self, numeric_caption:torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"Generiert onehot encodings aus Output von `numericalize`\n",
    "            Args:\n",
    "                numeric_caption (torch.Tensor): Tensor, welcher durch 'numericalize' erstellt wurde\n",
    "\n",
    "            Returns:\n",
    "                Onehot encoding als Tensor\n",
    "            \"\"\"\n",
    "\n",
    "            return torch.zeros((list(numeric_caption.shape) + [len(self)])).scatter_(len(numeric_caption.shape), numeric_caption.unsqueeze(len(numeric_caption.shape)).type(torch.int64), 1)\n",
    "        \n",
    "        def oneHot_sequence_to_tokens(self, sequence:torch.Tensor):\n",
    "            \"\"\"Inverse von `numerical_to_matrix`\n",
    "            Args:\n",
    "                sequence (torch.Tensor): onehot encodings\n",
    "\n",
    "            Returns:\n",
    "                liste von tokens als string\n",
    "            \"\"\"\n",
    "            \n",
    "            argmax = sequence.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            tokens = []\n",
    "            for arg in argmax:\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "        \n",
    "        def numerical_to_tokens(self, sequence:torch.Tensor):\n",
    "            \"\"\" Inverse von `numericalize`\n",
    "            Args:\n",
    "                sequence (torch.Tensor): sequenz von numerischen tokens (output von `oneHot_sequence_to_tokens`)\n",
    "\n",
    "            Returns:\n",
    "                liste von tokens als string\n",
    "            \"\"\"\n",
    "            \n",
    "            tokens = []\n",
    "            for arg in sequence.numpy():\n",
    "                tokens.append(self.index_to_token[arg])\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def __len__(self):\n",
    "            \"\"\"Länge des vocabs\n",
    "            \"\"\"\n",
    "            return len(self.index_to_token)\n",
    "            \n",
    "        @staticmethod\n",
    "        def tokenize_text(text:str) -> list:\n",
    "            \"\"\"Konvertiert text zu einer liste aus tokens\n",
    "            \"\"\"\n",
    "            return [token for token in text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).strip().split(\" \")]\n",
    "\n",
    "    class FlickrDataset(Dataset):\n",
    "        def __init__(self, img_root_dir:str, captions_file:str, img_transform=None, train_amount:float=0.8, valid_amount:float=0.1, split_random_state:int=1234, min_token_count:int=3, train_augmentation_multiplier:int=0) -> None:\n",
    "            super().__init__()\n",
    "            self.img_root_dir = img_root_dir #ordner mit bilder\n",
    "            self.captions_file = captions_file #file mit captions\n",
    "            self.img_transform = img_transform #transformation der bilder\n",
    "\n",
    "            self.captions = pd.read_csv(self.captions_file, header=0, names=[\"img\", \"caption\"]) #lese die captions ein\n",
    "\n",
    "            #instanziere tokenizer und erstelle vocab\n",
    "            self.tokenizer = Tokenizer(min_token_count=min_token_count)\n",
    "            self.tokenizer.build_vocab(self.captions.caption.values)\n",
    "\n",
    "            self.captions = self.captions.groupby(\"img\").caption.apply(list).to_frame().reset_index() #gruppiere nach den images, da jedes Image mehrere captions besitzt\n",
    "\n",
    "            #make train validation test split\n",
    "            self.train_captions = self.captions.sample(frac=train_amount, random_state=split_random_state) #sample train\n",
    "            self.valid_caption = self.captions.drop(self.train_captions.index).sample(frac=1/(1-train_amount)*valid_amount, random_state=split_random_state) #sample validation\n",
    "            self.test_caption = self.captions.drop(self.train_captions.index).drop(self.valid_caption.index) #sample test\n",
    "\n",
    "            #totale länge\n",
    "            tot_len = len(self.train_captions) + len(self.valid_caption) + len(self.test_caption)\n",
    "\n",
    "            print(f\"Dataset was fractioned into:\\n- train: {len(self.train_captions)/tot_len}\\n- validation: {len(self.valid_caption)/tot_len}\\n- test: {len(self.test_caption)/tot_len}\")\n",
    "            \n",
    "            self.captions = self.train_captions.explode(\"caption\") #überschreibe captions mit trainings captions\n",
    "\n",
    "            self.use_augmentation = False #init with false\n",
    "\n",
    "            #if augmentation enabled\n",
    "            if train_augmentation_multiplier > 0:\n",
    "                self.captions = pd.concat([self.captions]*train_augmentation_multiplier, axis=0) #duplicate dataframe n times\n",
    "                self.use_augmentation = True\n",
    "                print(f\"Data augmentation on training dataset enabled. Trainset is now {train_augmentation_multiplier} times its original size\")\n",
    "\n",
    "            self.is_test = False #only set if test\n",
    "\n",
    "        def get_validation(self):\n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.valid_caption.explode(\"caption\") #overrdie captions with valid_caption explosion\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            return ds_c\n",
    "        \n",
    "        def get_test(self):\n",
    "            ds_c = copy.copy(self) #make a copy of itself\n",
    "            ds_c.captions = self.test_caption #overrdie captions with test_captions\n",
    "            ds_c.use_augmentation = False #disable augmentation\n",
    "\n",
    "            ds_c.is_test = True #set test to true\n",
    "\n",
    "            return ds_c\n",
    "                \n",
    "        def augmentate_image(self, img:np.array):\n",
    "            \"\"\"Augmentiert bild mit zufälligen verschiedenen transformationen wie (blur, flip, rotation, pixel dropping, hue & saturation)\n",
    "            \"\"\"\n",
    "            aug = iaa.Sequential([\n",
    "                iaa.Sometimes(0.5, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Affine(rotate=(-20, 20), mode='symmetric'), \n",
    "                iaa.Sometimes(0.5,\n",
    "                            iaa.OneOf([iaa.Dropout(p=(0, 0.1)),\n",
    "                                        iaa.CoarseDropout(0.1, size_percent=0.5)])),\n",
    "                iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "            ])\n",
    "\n",
    "            return aug.augment_image(img) \n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.captions)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "\n",
    "            #get caption(s) and image id from dataframe\n",
    "            img_id, caption = self.captions.iloc[idx]\n",
    "\n",
    "            #read image from storage\n",
    "            img = Image.open(os.path.join(self.img_root_dir, img_id))\n",
    "\n",
    "            #if augmentation enabled -> do augmentation\n",
    "            if self.use_augmentation:\n",
    "                img = Image.fromarray(self.augmentate_image(np.array(img)))\n",
    "            \n",
    "            #if transformation enabled\n",
    "            if self.img_transform is not None:\n",
    "                img = self.img_transform(img)\n",
    "\n",
    "            if self.is_test: #caption is a list of captions\n",
    "                return img, caption\n",
    "            \n",
    "            else: #it is a single caption\n",
    "                numericalized_caption = torch.cat((torch.tensor([self.tokenizer.tok_to_index[\"<SOS>\"]]), self.tokenizer.numericalize(caption), torch.tensor([self.tokenizer.tok_to_index[\"<EOS>\"]])))\n",
    "                return img, numericalized_caption\n",
    "        \n",
    "    class Collate:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch]\n",
    "            targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx) #füge paddings hinzu\n",
    "\n",
    "            targets = self.dataset.tokenizer.numerical_to_matrix(targets) #sequenz aus tokens (ids) zu onehot\n",
    "\n",
    "            return imgs, targets\n",
    "        \n",
    "    class Collate_Test:\n",
    "        def __init__(self, dataset:FlickrDataset):\n",
    "            self.dataset = dataset\n",
    "            self.pad_idx = self.dataset.tokenizer.tok_to_index[\"<PAD>\"]\n",
    "\n",
    "        def __call__(self, batch):\n",
    "            imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "            imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "            targets = [item[1] for item in batch] #verändere targets nicht\n",
    "\n",
    "            return imgs, targets\n",
    "    \n",
    "    def get_loader(img_root_dir, captions_file, img_transform=None, batch_size=64, train_amount=0.8, valid_amount=0.1, min_token_count=3, train_augmentation_multiplier=0, num_workers_train:int=16):\n",
    "        #erstelle datenset\n",
    "        train_dataset = FlickrDataset(img_root_dir, captions_file, img_transform, train_amount=train_amount, valid_amount=valid_amount, min_token_count=min_token_count, train_augmentation_multiplier=train_augmentation_multiplier)\n",
    "        \n",
    "        #hole validations dataset\n",
    "        valid_dataset = train_dataset.get_validation()\n",
    "\n",
    "        #hole test datenset\n",
    "        test_dataset = train_dataset.get_test()\n",
    "\n",
    "        #erstelle dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(train_dataset),\n",
    "            num_workers=num_workers_train,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        validation_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate(valid_dataset),\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            collate_fn=Collate_Test(test_dataset),\n",
    "        )\n",
    "\n",
    "        return train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "#temporary outsource dataset stuff into file (otherwise num_workers of trainloader wont't work)\n",
    "with open(f'./tmp_ksdbf97skd.py', 'w') as file:\n",
    "    file.write(\"\\n\".join([line[4:] for line in inspect.getsource(tmp).split(\"\\n\")[1:]]))\n",
    "\n",
    "from tmp_ksdbf97skd import Tokenizer, FlickrDataset, Collate, Collate_Test, get_loader #use outsourced function and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Linear(vocab_size, embedding_size) #embeddings stored here\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size) #final output\n",
    "\n",
    "    def forward(self, image_embedding:torch.Tensor, onehot_words:torch.Tensor):\n",
    "        \"\"\" Forward propagation\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embeddign für Bild (dim: batchsize x embedding_size)\n",
    "            onehot_words (torch.Tensor): sequenz für onehot tokens (dim: batchsize x sequence len x vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            output der Sequenz mit dimensionen (batchsize x 1 [wegen Bild] + sequence len x embedding size)\n",
    "        \"\"\"\n",
    "\n",
    "        image_embedding = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "        word_embeddings = self.embedding(onehot_words) #embed onehot words\n",
    "\n",
    "        embeddings = torch.cat((image_embedding, word_embeddings), dim=1) #concat image embedding and word embeddings along sequence dim \n",
    "\n",
    "        x, _ = self.lstm(embeddings) #forward durch lstm\n",
    "        x = self.fc(x) #transformiere zurück von embedding size zu onehot size\n",
    "        x = F.log_softmax(x, dim=2) #wegen nlloss\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample_greedy(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=10):\n",
    "        \"\"\"Captioning eines Bildes mit greedy Methode. Sobald der stop token gefunden wurde, wird das captioning gestoppt.\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embedding des Bilds (dim: 1 x embedding_size)\n",
    "            start_token_onehot (torch.Tensor): one hot encoding des start tokens (dim: onehot size)\n",
    "            stop_token_onehot (torch.Tensor): one hot encoding des stop tokens (dim: onehot size)\n",
    "            max_num_words (int): maximale länge der caption\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = [] #predicted words stored here\n",
    "\n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            _, one_hot, c = self._step_next_token(self.embedding(start_token_onehot.unsqueeze(0).unsqueeze(0)), c) #feed embedding of start token (add first two dimension to start token) into network\n",
    "\n",
    "            #if stop token predicted\n",
    "            if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                if len(prediction) == 0:\n",
    "                    return torch.empty([])\n",
    "            \n",
    "                return torch.stack(prediction)\n",
    "        \n",
    "            prediction.append(one_hot[0, 0, :]) #add first predicted word to prediction list\n",
    "\n",
    "            for i in range(max_num_words):\n",
    "                input = self.embedding(one_hot) #embed word\n",
    "\n",
    "                _, one_hot, c = self._step_next_token(input, c) # predict next token based on last token and last c\n",
    "\n",
    "                #if stop token predicted\n",
    "                if torch.equal(one_hot[0, 0, :], stop_token_onehot):\n",
    "                    if len(prediction) == 0:\n",
    "                        return torch.empty([])\n",
    "                \n",
    "                    return torch.stack(prediction)\n",
    "                \n",
    "                prediction.append(one_hot[0, 0, :])\n",
    "\n",
    "            return torch.stack(prediction)\n",
    "\n",
    "    def sample_beamSearch(self, image_embedding:torch.Tensor, start_token_onehot:torch.Tensor, stop_token_onehot:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\"Captioning eines Bildes mit beamsearch Methode.\n",
    "        Args:\n",
    "            image_embedding (torch.Tensor): embedding des Bilds (dim: 1 x embedding_size)\n",
    "            start_token_onehot (torch.Tensor): one hot encoding des start tokens (dim: onehot size)\n",
    "            stop_token_onehot (torch.Tensor): one hot encoding des stop tokens (dim: onehot size)\n",
    "            max_num_words (int): maximale länge der caption\n",
    "            beam_size (int): grösse des Beams\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            global_best_prob = torch.tensor(-torch.inf) #best probability found stored here\n",
    "            \n",
    "            input = image_embedding.unsqueeze(1) #add a sequence dimension\n",
    "\n",
    "            _, _, c = self._step_next_token(input) #feed image into network (output is not important, long and short term memory is important)\n",
    "\n",
    "            def beamsearch(last_token_oneHot:torch.Tensor, last_prob:float, c:tuple, n:int=0) -> Tuple[List[int], float]:\n",
    "                nonlocal global_best_prob #global best probability\n",
    "\n",
    "                n += 1 #add n\n",
    "\n",
    "                input = self.embedding(last_token_oneHot)#embed\n",
    "\n",
    "                #forward pass through network\n",
    "                x, c = self.lstm(input, c) \n",
    "                x = x[:, [-1], :]\n",
    "                x = self.fc(x)\n",
    "                x = torch.log_softmax(x, dim=2) #log softmax\n",
    "\n",
    "                best_k_idx = torch.topk(x, beam_size, dim=2, largest=True).indices.flatten() #get indices of best predictions\n",
    "                best_k_prob = x[0, 0, best_k_idx] #get log probabilities of best predictions\n",
    "\n",
    "                #iterate over best prediction (idx with corresponding prob)\n",
    "                best_prediction = None\n",
    "                for idx, prob_t in torch.stack((best_k_idx, best_k_prob), dim=1):\n",
    "                    prob_current = last_prob+prob_t #calculate new probability \n",
    "\n",
    "                    #stop searching if prob_current already more negative than currently best -> because it can only get more negative or better say worse\n",
    "                    if prob_current < global_best_prob:\n",
    "                        continue\n",
    "                    \n",
    "                    #if we land here a token found which describes the image better\n",
    "                    idx_as_onehot = torch.zeros_like(x).scatter_(2, idx.unsqueeze(0).unsqueeze(0).unsqueeze(0).type(torch.int64), 1) #calculate onehot encoded token\n",
    "\n",
    "                    #if stop token predicted or max number of terms reached\n",
    "                    if torch.equal(idx_as_onehot[0, 0, :], stop_token_onehot) or n > max_num_words:\n",
    "                        #new best scorer found\n",
    "                        global_best_prob = prob_current\n",
    "                        best_prediction = []\n",
    "\n",
    "                    else:\n",
    "                        best_tokens_next = beamsearch(idx_as_onehot, prob_current, c, n) #end token not found yet\n",
    "\n",
    "                        #only returns != None if a better caption found\n",
    "                        if best_tokens_next != None:\n",
    "                            best_prediction = [idx] + best_tokens_next #append token to this caption\n",
    "\n",
    "                return best_prediction\n",
    "\n",
    "            best_prediction = beamsearch(start_token_onehot.unsqueeze(0).unsqueeze(0), 0, c, 0) #feed embedding of start token (add first two dimension to start token) into network -> best prediction\n",
    "\n",
    "            return torch.Tensor(best_prediction).flatten().type(torch.int)\n",
    "\n",
    "    def _step_next_token(self, input:torch.Tensor, c:tuple=None):\n",
    "        \"\"\" Predicted nächsten token \n",
    "        Args:\n",
    "            input (torch.Tensor): embedded token (dim: 1 x sequence_len x embedding_size)\n",
    "            c (tuple): hidden state\n",
    "\n",
    "        Returns:\n",
    "            (log softmax des predicteten tokens, one hot encoded token, hidden state c)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            x, c = self.lstm(input, c) \n",
    "            x = x[:, [-1], :]\n",
    "            x = self.fc(x)\n",
    "            max_idx = torch.argmax(x, dim=2)\n",
    "            one_hot = torch.zeros_like(x).scatter_(2, max_idx.unsqueeze(2), 1)\n",
    "\n",
    "            return F.log_softmax(x, dim=2), one_hot, c\n",
    "\n",
    "def make_cnn_resnet50(embedding_size:int):\n",
    "    \"\"\"Make resnet50 with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.fc = nn.Sequential(nn.BatchNorm1d(4*512), nn.Linear(4*512, embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_cnn_densenet201(embedding_size:int):\n",
    "    \"\"\"Make densenet with last layer replaced\n",
    "    \"\"\"\n",
    "    cnn = densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    for param in cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    cnn.classifier = nn.Sequential(nn.BatchNorm1d(cnn.classifier.weight.shape[1]), nn.Linear(cnn.classifier.weight.shape[1], embedding_size), nn.BatchNorm1d(embedding_size)) #replace last layer\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def make_rnn_lstm(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    \"\"\"Make lstm \n",
    "    \"\"\"\n",
    "    return RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "\n",
    "def make_rnn_gru(embedding_size:int, hidden_size:int, vocab_size:int, dropout:float=0, num_layers:int=2):\n",
    "    \"\"\"Make rnn with lstm replaced with gru\n",
    "    \"\"\"\n",
    "    gru = RNN(embedding_size, hidden_size, vocab_size, dropout, num_layers=num_layers)\n",
    "    gru.lstm = nn.GRU(embedding_size, hidden_size, dropout=dropout, num_layers=num_layers, batch_first=True) #replace lstm with gru layer\n",
    "    return gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, train_dataset:FlickrDataset, embedding_size:int=100, hidden_size:int=100, lstm_dropout:float=0, num_lstm_layers:int=2, cnn_type:str=\"resnet50\", rnn_type:str=\"lstm\", test_eval_step_pred_max_sequLen:int=30, test_step_pred_beamsize:int=3, alpha:float=0.0001):\n",
    "        \"\"\"Lightning modul des Netzes\n",
    "        Args:\n",
    "            train_dataset (FlickrDataset): Datenset (hier wird allerdings nur der Tokenizer benötigt)\n",
    "            embedding_size (int): grösse der embeddings\n",
    "            hidden_size (int): grösse der hidden size (cell state / hidden state)\n",
    "            lstm_dropout (float): dropout probability between lstm layers (if num_lstm_layers > 1)\n",
    "            num_lstm_layers (int): anzahl lstm layers\n",
    "            cnn_type (str): typ des image encoders ('resnet50' oder 'densenet201')\n",
    "            rnn_type (str): typ des rnn ('lstm' oder 'gru')\n",
    "            test_eval_step_pred_max_sequLen (int): max länge der prediction einer caption während des tests sowie validierung\n",
    "            test_step_pred_beamsize (int): beam size des beamsearch während des tests\n",
    "            alpha (float): regularisierung\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.test_eval_step_pred_max_sequLen = test_eval_step_pred_max_sequLen\n",
    "        self.test_step_pred_beamsize = test_step_pred_beamsize\n",
    "        self.tokenizer = train_dataset.tokenizer\n",
    "        self.tokenizer_min_token_count = self.tokenizer.min_token_count\n",
    "        self.data_augmentation_enabled = train_dataset.use_augmentation\n",
    "        self.alpha = alpha\n",
    "\n",
    "        #load specific cnn\n",
    "        if cnn_type == \"resnet50\":\n",
    "            self.cnn = make_cnn_resnet50(embedding_size)\n",
    "\n",
    "        elif cnn_type == \"densenet201\":\n",
    "            self.cnn = make_cnn_densenet201(embedding_size)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"unknown cnn_type\")\n",
    "        \n",
    "        #load specific rnn\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = make_rnn_lstm(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_lstm_layers)\n",
    "\n",
    "        elif rnn_type == \"gru\":\n",
    "            self.rnn = make_rnn_gru(embedding_size, hidden_size, len(self.tokenizer), lstm_dropout, num_lstm_layers)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"unknown rnn_type\")\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def training_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Training step für lightning\n",
    "        \"\"\"\n",
    "\n",
    "        loss = self._get_teacherForce_loss(batch) #calculate loss based on batch\n",
    "         \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True) #log loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Validation step für lightning\n",
    "        \"\"\"\n",
    "        # loss_greedy not logged anymore since the bleu loss doesn't get much better after a couple epochs and uses much compute time\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_tf = self._get_teacherForce_loss(batch) #calculate teacher force log loss based on batch\n",
    "            #loss_greedy = self._get_greedy_bleu_loss(batch) #calculate greedy log loss based on batch\n",
    "        \n",
    "        self.log(\"val_loss\", loss_tf, on_step=True, on_epoch=True) #negative log loss of teacher forcing\n",
    "        #self.log(\"val_loss_bleu\", loss_greedy, on_step=True, on_epoch=True) #bleu loss of greedy prediction\n",
    "\n",
    "    def test_step(self, batch:tuple, batch_idx):\n",
    "        \"\"\"Test step für lightning\n",
    "        \"\"\"\n",
    "        batch_image, batch_captions = batch #split des inputs\n",
    "\n",
    "        list_bleu_greedy = [] #hier werden die bleu scores mit greedy für alle bilder des batches abgespeichert\n",
    "        list_bleu_beamSearch = [] #hier werden die bleu scores mit beamsearch für alle bilder des batches abgespeichert\n",
    "        #iteriere über batch\n",
    "        for i in range(len(batch_image)):\n",
    "            image = batch_image[i] #hole einzelnes bild\n",
    "            captions = [self.tokenizer.tokenize_text(caption) for caption in batch_captions[i]] #tokenize die einzelnen captions des bildes\n",
    "\n",
    "            #führe ein captioning des bildes durch. Einmal mit greedy einmal mit beamsearch\n",
    "            pred_greedy = self.sample_greedy(image, self.test_eval_step_pred_max_sequLen)\n",
    "            pred_beamsearch = self.sample_beamSearch(image, self.test_eval_step_pred_max_sequLen, self.test_step_pred_beamsize)\n",
    "\n",
    "            #bleu scores berechnen\n",
    "            bleu_greedy = sentence_bleu(captions, pred_greedy)\n",
    "            bleu_beamSearch = sentence_bleu(captions, pred_beamsearch)\n",
    "\n",
    "            #store in list\n",
    "            list_bleu_greedy.append(bleu_greedy)\n",
    "            list_bleu_beamSearch.append(bleu_beamSearch)\n",
    "\n",
    "        #logge mittelwert\n",
    "        self.log(\"test_bleu_greedy\", np.mean(list_bleu_greedy), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu greedy\n",
    "        self.log(\"test_bleu_beamSearch\", np.mean(list_bleu_beamSearch), on_step=True, on_epoch=True, batch_size=len(batch_image)) #log bleu greedy\n",
    "\n",
    "    def _get_teacherForce_loss(self, batch:tuple):\n",
    "        \"\"\"Calculate loss (teacher forced) based on batch$\n",
    "        Args:\n",
    "            batch (tuple): output von dataloader\n",
    "        Returns:\n",
    "            negative log loss von batch\n",
    "        \"\"\"\n",
    "\n",
    "        #type definition\n",
    "        batch_images:torch.Tensor\n",
    "        batch_targets_onehot:torch.Tensor\n",
    "\n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding = self.cnn(batch_images) #get image embeddings for whole batch\n",
    "\n",
    "        output:torch.Tensor = self.rnn(image_embedding, batch_targets_onehot[:, :-1, :]) #feed image and tokens into rnn (last word is not sent to rnn since this is only the stop token)\n",
    "\n",
    "        output = output[:, 1:, :] #ignore first sequence from output since this is the prediction if the image\n",
    "        batch_targets_onehot = batch_targets_onehot[:, 1:, :] #ignore first sequence from tokens since this is the start token\n",
    "\n",
    "        #remove batch dimension\n",
    "        batch_targets_onehot = batch_targets_onehot.flatten(end_dim=1) #ignore start token\n",
    "        output = output.flatten(end_dim=1)\n",
    "        \n",
    "        #nll loss with padding tokens ignored\n",
    "        loss = F.nll_loss(output, batch_targets_onehot.argmax(dim=1), ignore_index=int(self.tokenizer.tok_to_index[\"<PAD>\"]))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _get_greedy_bleu_loss(self, batch:tuple):\n",
    "        \"\"\"Calculate bleu loss (simply 1 - bleu score).\n",
    "        Args:\n",
    "            batch (tuple): output von dataloader\n",
    "        Returns:\n",
    "            mittlerer bleu loss des batches\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_images, batch_targets_onehot = batch #split batch data into image and onehot encodings for targets\n",
    "\n",
    "        image_embedding:torch.Tensor = self.cnn(batch_images) #get image embeddings for whole batch and add a sequence dimension\n",
    "\n",
    "        #get start and stop token\n",
    "        start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "        stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "        #iterate over batch\n",
    "        bleu_losses = [] #bleu losses stored here\n",
    "        for i in range(len(batch_images)):\n",
    "            image:torch.Tensor = image_embedding[[i]] #get image from batch\n",
    "            targets_onehot:torch.Tensor = batch_targets_onehot[i] #get image from batch\n",
    "            \n",
    "            #transform onehot tokens back to string tokens (ignore special tokens)\n",
    "            reference = list(filter(lambda a: a not in ['<SOS>', '<EOS>', '<PAD>'], self.tokenizer.oneHot_sequence_to_tokens(targets_onehot)))\n",
    "            \n",
    "            #predict string tokens from greedy prediction\n",
    "            prediction = self.tokenizer.oneHot_sequence_to_tokens(self.rnn.sample_greedy(image, start_token_onehot, stop_token_onehot, self.test_eval_step_pred_max_sequLen))\n",
    "\n",
    "            #add bleu loss to list\n",
    "            bleu_losses.append(1-sentence_bleu([reference], prediction))\n",
    "        \n",
    "        return np.mean(bleu_losses) #return mean bleu loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), weight_decay=self.alpha)\n",
    "        return optimizer\n",
    "        \n",
    "    def sample_greedy(self, image:torch.Tensor, max_num_words:int=10):\n",
    "        \"\"\" Predict caption mit greedy methode\n",
    "        Args:\n",
    "            image (torch.Tensor): Bild als Tensor\n",
    "            max_num_words (int): maximale anzahl an tokens der prediction\n",
    "\n",
    "        Returns:\n",
    "            caption as string tokens\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0)) #embedd image\n",
    "\n",
    "            #get start and stop token\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            #make a greedy prediction\n",
    "            output = self.rnn.sample_greedy(image_embedding, start_token_onehot, stop_token_onehot, max_num_words)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output.size()) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.oneHot_sequence_to_tokens(output.cpu()) #convert onehot tokens to string tokens\n",
    "        \n",
    "    def sample_beamSearch(self, image:torch.Tensor, max_num_words:int=3, beam_size:int=2):\n",
    "        \"\"\" Predict caption mit beamsearch methode\n",
    "        Args:\n",
    "            image (torch.Tensor): Bild als Tensor\n",
    "            max_num_words (int): maximale anzahl an tokens der prediction\n",
    "            beam_size (int): beam size\n",
    "\n",
    "        Returns:\n",
    "            caption as string tokens\n",
    "        \"\"\"\n",
    "\n",
    "        self.eval() #set to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding:torch.Tensor = self.cnn(image.unsqueeze(0))\n",
    "\n",
    "            start_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<SOS>\"])).to(image_embedding.device)\n",
    "            stop_token_onehot = self.tokenizer.numerical_to_matrix(torch.tensor(self.tokenizer.tok_to_index[\"<EOS>\"])).to(image_embedding.device)\n",
    "\n",
    "            output = self.rnn.sample_beamSearch(image_embedding, start_token_onehot, stop_token_onehot, max_num_words, beam_size)\n",
    "\n",
    "            #if output is empty\n",
    "            if len(output) == 0:\n",
    "                return \"<empty>\"\n",
    "\n",
    "            return self.tokenizer.numerical_to_tokens(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data\n",
    "batchsize = 64\n",
    "min_token_count = 3\n",
    "train_augmentation_multiplier = 6\n",
    "img_resize = 224\n",
    "num_workers_train = 8\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((img_resize, img_resize), antialias=True), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_loader, validation_loader, test_loader, train_dataset, valid_dataset, test_dataset = get_loader(r\"C:\\Users\\tobia\\archive\\Images\", r\"C:\\Users\\tobia\\archive\\captions.txt\", transform, batchsize, min_token_count=min_token_count, train_augmentation_multiplier=train_augmentation_multiplier, num_workers_train=num_workers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load model from checkpoint\n",
    "model_run_id = \"model-if28cok5:best\"\n",
    "artifact_dir = wandb.Api().artifact(f\"{user}/{project}/{model_run_id}\").download()\n",
    "\n",
    "# load checkpoint\n",
    "model = LitModel.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model in a single run\n",
    "\n",
    "#create fresh model\n",
    "embedding_size = 512\n",
    "hidden_size = 1024\n",
    "lstm_dropout = 0.5\n",
    "lstm_num_layers = 3\n",
    "cnn_type = \"densenet201\"\n",
    "rnn_type = \"lstm\"\n",
    "test_step_pred_max_sequLen = 30\n",
    "test_step_pred_beamsize = 3\n",
    "alpha = 0.0001\n",
    "\n",
    "model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, lstm_num_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "#start training\n",
    "logger = WandbLogger(project=project, log_model=\"all\")\n",
    "logger.watch(model, log=\"all\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=2, verbose=False, mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##train model in a sweep\n",
    "\n",
    "def sweep_iteration():\n",
    "    wandb.init()\n",
    "\n",
    "    #create fresh model\n",
    "    embedding_size = wandb.config.embedding_size\n",
    "    hidden_size = wandb.config.hidden_size\n",
    "    lstm_dropout = wandb.config.lstm_dropout\n",
    "    lstm_num_layers = wandb.config.lstm_num_layers\n",
    "    cnn_type = wandb.config.cnn_type\n",
    "    rnn_type = wandb.config.rnn_type\n",
    "    test_step_pred_max_sequLen = 30\n",
    "    test_step_pred_beamsize = 3\n",
    "    alpha = wandb.config.alpha\n",
    "\n",
    "    model = LitModel(train_dataset, embedding_size, hidden_size, lstm_dropout, lstm_num_layers, cnn_type, rnn_type, test_step_pred_max_sequLen, test_step_pred_beamsize, alpha)\n",
    "\n",
    "    #start training\n",
    "    logger = WandbLogger(project=project, log_model=\"all\")\n",
    "    logger.watch(model, log=\"all\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=2, verbose=False, mode=\"min\")\n",
    "\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"gpu\", logger=logger, callbacks=[early_stop_callback, checkpoint_callback], log_every_n_steps=50)\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n",
    "    trainer.test(ckpt_path=\"best\", dataloaders=test_loader)\n",
    "\n",
    "    best_val_loss = early_stop_callback.state_dict()[\"best_score\"] #get best validation loss from early stopper\n",
    "\n",
    "    wandb.log({\"best_val_loss\":best_val_loss}) #log best validation loss\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"name\": \"test_sweep\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"minimize\",\n",
    "        \"name\": \"best_val_loss\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"embedding_size\": {\"values\":[16, 128, 512]},\n",
    "        \"hidden_size\": {\"values\":[16, 128, 512]},\n",
    "        \"lstm_dropout\": {\"values\":[0.5]},\n",
    "        \"lstm_num_layers\": {\"values\":[3]},\n",
    "        \"cnn_type\": {\"values\":[\"resnet50\", \"densenet201\"]},\n",
    "        \"rnn_type\": {\"values\":[\"lstm\"]},\n",
    "        \"alpha\": {\"values\":[0.0001]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "wandb.agent(sweep_id, sweep_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show multiple images\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "max_num_words = 30\n",
    "for i in range(0, 100):\n",
    "    image, captions = ds.__getitem__(i) #get item\n",
    "\n",
    "    imshow(image)\n",
    "    print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "    print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "    print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show single image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "ds = test_dataset #choose dataset\n",
    "\n",
    "image, captions = ds.__getitem__(18) #get item\n",
    "\n",
    "imshow(image)\n",
    "print(\"Reference captions:\\n\", \"\\n\".join([str(ds.tokenizer.tokenize_text(caption)) for caption in captions]))\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "root = tkinter.Tk()\n",
    "root.withdraw()\n",
    "file_path = tkinter.filedialog.askopenfilename()\n",
    "root.destroy()\n",
    "\n",
    "image = Image.open(file_path)\n",
    "image = transform(image)\n",
    "\n",
    "imshow(image)\n",
    "\n",
    "max_num_words = 30\n",
    "print(\"greedy caption: \", model.sample_greedy(image.cuda(), max_num_words=max_num_words))\n",
    "print(\"beamsearch caption: \", model.sample_beamSearch(image.cuda(), max_num_words=max_num_words, beam_size=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
